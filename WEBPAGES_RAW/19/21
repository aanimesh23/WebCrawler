<html>
<head>
  <meta http-equiv="Content-Language" content="en-us">
  <meta http-equiv="content-type" content="text/html; charset=windows-1252">
  <title>CS 221 - Information Retrieval</title>
  <style>
<!--
table.MsoTableGrid
{border:1.0pt solid windowtext;
font-size:10.0pt;
font-family:Times;
}
-->

  </style>
</head>

<body bgcolor="#FFFFFF">
<h2><font face="Pristina">CS 221</font>
<font face="Pristina">Information Retrieval</font></h2>

<table border="1" cellpadding="10" cellspacing="0"
style="border-collapse: collapse" width="100%" id="AutoNumber1">
  <tbody>
    <tr>
      <td width="20%">
        <table border="0" cellpadding="0" cellspacing="0"
        style="border-collapse: collapse" width="97%" id="AutoNumber2">
          <tbody>
<!--
	    <tr>
              <td width="100%"><p><strong>Groupware</strong></p>

                * <a href="http://getsatisfaction.com/uc_irvine">Discussions</a></br>
                * <a href="http://ir-winter2010.blogspot.com/">Course
                Blog</a>

             </td>
            </tr>
          <tr>
              <td width="100%">&nbsp;</td>
            </tr>
           <tr>
              <td width="100%">&nbsp;</td>
            </tr>
-->
           <tr>
              <td width="100%"></td>
            </tr>
            <tr>
              <td width="100%"><b><font face="Arial" size="2"><a
                href="#homework">Homework Projects</a></font></b></td>
            </tr>
           <tr>
              <td width="100%"><b><font face="Arial" size="2"><a
                href="#summaries">Paper Summaries</a></font></b></td>
            </tr>
            <tr>
              <td width="100%"><b><font face="Arial" size="2"><a
                href="#syllabus">Syllabus</a></font></b></td>
            </tr>
            <tr>
              <td width="100%">&nbsp;</td>
            </tr>
            <tr>
              <td width="100%"><b><font face="Arial" size="2"><a
                href="#honesty">Academic Honesty</a></font></b></td>
            </tr>
            <tr>
              <td width="100%"><font face="Arial" size="2">&nbsp;</font></td>
            </tr>
            <tr>
              <td width="100%"><b><font face="Arial" size="2"><a
                href="#disability">Students with Disability</a></font></b></td>
            </tr>
          </tbody>
        </table>
      </td>
      <td width="80%" valign="top"><font size="4"
        face="Pristina"><b>Synopsis</b></font> 

        <p><b><font face="Pristina">Purpose</font><font
        size="2">.</font></b><font size="2"></font> <font face="Pristina">An
        introduction to information retrieval including indexing, retrieval,
        classifying, and clustering text and multimedia documents.</font></p>

        <p><font face="Pristina"><b>Book</b></font><font size="2">. <font
        size="-1"><span style="font-size: 12pt"><span
        style="font-size: 11pt"><a
        href="http://nlp.stanford.edu/IR-book/information-retrieval-book.html">Introduction
        to Information Retrieval</a> by Christopher D. Manning, Prabhakar
        Raghavan and Hinrich Schutze </span></span></font></font></p>

       <p><b><font face="Pristina">Evaluation. </font></b>
        Homework/lab projects (1/2) + Summaries (1/4) + Quizzes (1/4)</p>

	<p><font face="Pristina"><b>Pedagogy:</b></font></br>
        - Lectures cover the material in the reading materials by placing it in
        context, giving examples, and engaging in Q&amp;As. <br>
       - Homework projects are hands-on vehicles for learning the material.
        Collaboration and knowledge exchange are encouraged in the projects, but mindless
        copy of solutions (aka cheating)</font> is not allowed.<br>
       - Papers cover the foundations of the field of Information
	Retrieval, right from the original source.</p>
      </td>
    </tr>
  </tbody>
</table>

<p><font face="Pristina"></font>Instructor: <a
href="http://www.ics.uci.edu/~lopes">Prof. Cristina Lopes</a>, DBH 5076, lopes
<i><font size="1">at</font></i> ics <i><font size="1">dot</font></i> uci
<i><font size="1">dot</font></i> edu <br>
<font face="Pristina">Reader</font>: <a
href="http://www.ics.uci.edu/">TBD</a></p>

<p><font face="Pristina">Lectures</font>: Tue &amp; Thu 9:30-10:50am,
ICS 174</br>
Office hours: Mondays and Wednesdays, 11am-12pm, ICS 408
</p>
<hr>

<a name="homework"><p><b><font size="4">Projects</font></b></p></a>

<p><a href="projects/projects.pdf">Project descriptions</a></p>

<!--
<p>Support for this courses's projects kindly provided by
<img width="100px" src="amazon-web-services-logo-large.png"></p>
-->


<p>There will be 3 projects, the last one with several milestones. Projects are due
<font color="#FF0000"><font face="Pristina">by midnight</font></font>
on the due date.
Late projects will be accepted with penalties.</p>

<p><b>Submission</b></p>

<p>See instructions in each project description.<br>
</p>

<p><b>Important dates</b></p>

<table border="1">
  <tr>
    <td>Assignment</td>
    <td>Topic</td>
    <td>Due date</td>
    <td>Weight</td>
  </tr>
  <tr>
    <td>1</td>
    <td><a href="assignments/Assignment1.pdf">Text processing</a></td>
    <td>1/20</td>
    <td>20%</td>
  </tr>
  <tr>
    <td>2</td>
    <td><a href="assignments/Project2.pdf">Web crawling</a></td>
    <td>2/3</td>
    <td>20%</td>
  </tr>
  <tr>
    <td>3</td>
    <td><a href="assignments/Project3.pdf">Search Engine</a></td>
    <td>2/17, 3/3, 3/15</td>
    <td>60%</td>
  </tr>
</table>


<p>&nbsp;</p>
<hr>

<a name="quizzes"><p><b><font size="4">Quizzes</font></b></p></a>

<p>There will be 4 quizzes throughout the course. <font color="#FF0000"
face="Pristina">Quizzes are on Tuesdays</font> during the lecture.
They cover material that has been taught the previous weeks since
the last quizz. The quiz with the worst score will be discarded.
No quiz make-ups.</p>

<table border="1">
  <tr>
    <td>Quiz</td>
    <td>Date</td>
  </tr>
  <tr>
    <td>1</td>
    <td>1/22</td>
  </tr>
  <tr>
    <td>2</td>
    <td>2/5</td>
  </tr>
  <tr>
    <td>3</td>
    <td>2/19</td>
  </tr>
  <tr>
    <td>4</td>
    <td>3/5</td>
  </tr>
</table>
<p>&nbsp;</p>
<hr>

<a name="summaries"><h3>Paper Summaries</h3></a>

<p> Summaries are due Fridays. Summaries submitted up to one week late
will have a penalty of 35%. No summaries will be accepted past 1 week
of their due date.</p>
<p>Each article should be summarized in no more than one page, with
the following structure: (a) objective summary of the article (do not
inject your views here, be objective); (b) short personal commentary
about the article (your views here).</p>
<p>Submit one pdf file per week with all the summaries for that week on that file.</p>
<p>Please name your paper summary files like this:</br>
<i>LastName</i>_<i>WeekNumber</i>.pdf</br>
starting with <i>WeekNumber</i>=1 for the first week.</br>

Files that don't follow this convention may be missed by the
instructors.</p>

<p>Include your full name and student ID in the summary itself.</p>

<p>Turn in summaries in EEE Dropbox.</p>

<hr>

<a name="syllabus"><h3>Syllabus:</h3></a>

<p></p>

<table border="1" style="width: 100%">
  <caption></caption>
  <col>
  <col>
  <col>
  <col>
  <col>
  <col>
  <tbody>
    <tr>
      <td>Week</td>
      <td>Date</td>
      <td>Topic </td>
      <td>Weekly materials</td>
      <td>Deliverables</td>
      <td>Notes</td>
    </tr>
    <tr>
      <td rowspan="2">1</td>
      <td>1/8</td>
      <td rowspan="2">Web Search Basics</td>
      <td rowspan="2">Textbook <a
        href="http://nlp.stanford.edu/IR-book/information-retrieval-book.html">Chapter
        19</a>: Web Search Basics (no need to summarize)

        <p>1. Wikipedia entry on <a
        href="http://en.wikipedia.org/wiki/Vannevar_Bush">Vannevar Bush</a></p>

        <p>2. "<a href="http://www.theatlantic.com/doc/194507/bush">As We May
        Think</a>" The Atlantic Monthly, July, 1945. (reprinted in ACM CHI
        Interactions, March 1996)</p>

     </td>
      <td rowspan="2">
	Summaries 
      </td>
      <td>
	<p><a href="slides/Lecture01Slides.pdf">Slides</a> </p>
      </td>
    </tr>
    <tr>
      <td>1/10</td>
      <td>
	<p><a href="slides/The Web.pdf">The Web</a></p>
        <p><a href="slides/Lecture02Slides.pdf">Slides</a></p>
      </td>
    </tr>
    <tr>
      <td rowspan="2">2</td>
      <td>1/15</td>
      <td rowspan="2">
 	<p>Text Processing</p>
	<p>Search Engine Optimization</p>
      </td>
      <td rowspan="2">
        <p>3. "<a href="http://doi.acm.org/10.1145/860435.860451">Stuff I've
        seen: A system for personal information retrieval and re-use</a> " by
        S. Dumais, E. Cutell, J. Cadiz, G. Jancke, R. Sarin, and D. Robbins,
        SIGIR, 2003</p>

        <p>Commentary: "This paper addresses an increasingly important problem
        - how to search and manage personal collections of electronic
        information. ... it addresses an important user-centered problem.
        ...this paper presents a practical user interface to make the system
        useful. ..., the paper includes large scale, user-oriented testing that
        demonstrates the efficacy of the system. ..., the evaluation uses both
        quantitative and qualitative data to make its case. I think this paper
        is destined to be a classic because it may eventually define how people
        manage their files for a decade. Moreover, it is well-written and can
        serve as a good model for developers doing system design and
        evaluation, and for students learning about IR systems and
        evaluation."</p>

	<p>4. "<a
        href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.53.8337">Simple,
        Proven Approaches to Text Retrieval</a>" by Robertson and Jones </p>

        <p>Commentary: "This paper provides a brief but well informed and
        technically accurate overview of the state of the art in text
        retrieval, at least up to 1997. It introduces the ideas of terms and
        matching, term weighting strategies, relevance weighting, a little on
        data structures and the evidence for their effectiveness. In my view it
        does an exemplary job of introducing the terminology of IR and the main
        issues in text retrieval for a numerate and technically well informed
        audience. It also has a very well chosen list of references."</p>

        <p></p>
      </td>
      <td rowspan="2">
	<p>Summaries </p></td>
      <td><a href="slides/TextProcessing.pdf">Slides</a>
        <p><a href="http://dg3rtljvitrle.cloudfront.net/slides/chap4.pdf">Slides</a></p>
      </td>
    </tr>
    <tr>
      <td>1/17</td>
      <td>
	<p><a href="slides/Lecture03Slides.pdf">Slides</a></p>
	<p><a href="slides/Lecture04Slides.pdf">Slides</a></p>
      </td>
    </tr>
    <tr>
      <td rowspan="2">3</td>
      <td>1/22</td>
      <td rowspan="2">Web crawling </td>
      <td rowspan="2">Textbook <a
        href="http://nlp.stanford.edu/IR-book/information-retrieval-book.html">Chapter
        20</a> : Web Crawling and Indices (no need to summarize)<br/>

        <p>5. "<a
        href="http://portal.acm.org/citation.cfm?id=335168.335170&amp;coll=ACM&amp;dl=ACM&amp;CFID=5772105&amp;CFTOKEN=18886490">The
        Web As a Graph</a>" by R. Kumar, P Raghavan, S. Rajagopalan, D.
        Sivakumar, A. Tomkins, E. Upfal, PODS 2000</p>

        <p>Abstract: "The pages and hyperlinks of the World-Wide Web may be
        viewed as nodes and edges in a directed graph. This graph has about a
        billion nodes today, several billion links, and appears to grow
        exponentially with time. There are many reasons -- mathematical,
        sociological, and commercial -- for studying the evolution of this graph.
        We first review a set of algorithms that operate on the Web graph,
        addressing problems from Web search, automatic community discovery, and
        classification. We then recall a number of measurements and properties
        of the Web graph. Noting that traditional random graph models do not
        explain these observations, we propose a new family of random graph
        models."</p>

        <p></p>
      </td>
      <td rowspan="2">
	Summaries  
      </td>
      <td><a href="slides/WebCrawling.pdf">Slides</a>
      </td>
    </tr>

    <tr>
      <td>1/24</td>
      <td><a href="slides/EthicsAndLaw.pdf">Slides</a>
        <p><a
        href="http://dg3rtljvitrle.cloudfront.net/slides/chap3.pdf">More</a></p>
      </td>
    </tr>
    <tr>
      <td rowspan="2">4</td>
      <td>1/29*</td>
      <td rowspan="2">Web Crawling</td>
      <td rowspan="2">
	<p>On Tuesday, 1/29 Prof. Chen Li will share his experiences
	  of doing search-related research and commercializing the results.  The
	  work is mainly conducted in the iPubmed project
	  (http://ipubmed.ics.uci.edu), which can support instant, fuzzy search
	  on more than 21 million medical publications.  He has been doing a
	  startup, called SRCH2, to commercialize the techniques.  The company
	  has spent the last few years developing a new full text search
	  software from the ground up.  Here are some of the things SRCH2 can
	  do: it resides wholly in-memory, deploys multi-threaded queries, uses
	  cached forward indexing to enable instant recommendations, does rapid
	  geo search, error correction, customizable rankings, with real-time
	  updates, all in parallel, in scale. Each feature addresses significant
	  pain points for our growing list of enterprise mobile, social, and
	  e-commerce clients. He will share experiences of doing research
	  commercialization.</p>
        <p>6. "<a href="http://swtch.com/~rsc/regexp/regexp4.html">How Google Code Search Worked
        </a> " by Russ Cox (January 2012)<br/>
        Commentary: Google code search has been a great resource for developers, but it
           has just been shut down. This blog post explains how it worked.</p>
	
     </td>
      <td rowspan="2">
	Summaries</td>
      <td>*invited lecturer</td>
    </tr>
    <tr>
      <td>1/31</td>
      <td><a href="slides/Lecture08.pdf">Slides</a> 
      </td>
    </tr>
    <tr>
      <td rowspan="2">5</td>
      <td>2/5</td>
      <td rowspan="2">Index Construction and Scoring</td>
     <td rowspan="2">Textbook <a
        href="http://nlp.stanford.edu/IR-book/information-retrieval-book.html">Chapter
        4</a> : Index Construction 
      <p>7. <a
      href="http://research.google.com/pubs/archive/35179.pdf">The
      unreasonable effectiveness of data</a></p>
        <p>Commentary: Three Google researchers summarize the benefits of
      data-driven problem-solving in an essay that borrows the title from
      <a
      href="http://www.dartmouth.edu/~matc/MathDrama/reading/Wigner.html">another
      famous paper that proposes the opposite</a>.</p>
     </td>
      <td rowspan="2">
	Summaries
      </td>
      <td>
          <a href="http://www.stanford.edu/class/cs276/handouts/lecture4-indexconstruction.ppt">Slides</a></td>
    </tr>
    <tr>
      <td>2/7</td>
      <td><a href="http://dg3rtljvitrle.cloudfront.net/slides/chap5.pdf">Compression</a><br/>
	<a href="slides/MapReduce.pdf">MapReduce</a><br/>
	<a href="slides/Hadoop-AWS.pdf">Hadoop</a>          
     </td>
    </tr>
    <tr>
      <td rowspan="2">6</td>
      <td>2/12</td>
      <td rowspan="2">Querying, Scoring, Term Weighting and the Vector Space
        model</td>
      <td rowspan="2">Textbook <a
        href="http://nlp.stanford.edu/IR-book/information-retrieval-book.html">Chapter
        1</a> : Boolean Retrieval 

        <p>Textbook <a
        href="http://nlp.stanford.edu/IR-book/information-retrieval-book.html">Chapter
        6</a> : Scoring, term weighting &amp; the vector space model</p>

        <p>8.<a href="http://dl.acm.org/citation.cfm?id=361220">A vector space model for automatic indexing</a>
           by Salton, Wong, Yang 
        </p>

        <p></p>
      </td>
      <td rowspan="2">
	Summaries
      </td>
      <td><a href="slides/Querying-1.ppt">Slides</a><br/>
	  <a href="slides/tfidf.ppt">Slides</a><br/>
          </td>
    </tr>
    <tr>
      <td>2/14</td>
      <td><a href="slides/Lecture14.pdf">Slides</a></td>
    </tr>
    <tr>
      <td rowspan="2">7</td>
      <td>2/19</td>
      <td rowspan="2">Search Engine Evaluation<br/>
	Vector Space Model
      </td>
      <td rowspan="2">
      <p>10. <a href="http://dl.acm.org/citation.cfm?id=1327492">"Map Reduce: Simplified Data Processing on
        Large Clusters"</a> by Jeffrey Dean and Sanjay Ghemawat</p>
       <p>Commentary: the paper that revolutionized modern data
        processing, made "cloud computing" trendy, and a great example of how programming language
        concepts can be applied to the design of real systems.</p>
        <p>&nbsp;</p>
      </td>
      <td rowspan="2">
       Summaries 
      </td>
      <td><a href="slides/chap8.pptx">Slides</a></td>
    </tr>
    <tr>
      <td>2/21</td>
      <td>
        <p><a href="slides/lecture7-vectorspace.ppt">Slides</a></p>
      </td>
    </tr>

    <tr>
      <td rowspan="2">8</td>
      <td>2/26</td>
      <td rowspan="2">Link Analysis</td>
      <td rowspan="2"><p> Textbook <a
        href="http://nlp.stanford.edu/IR-book/information-retrieval-book.html">Chapter
        21</a> : Link Analysis</p>
        <p>9. "<a href="http://www-db.stanford.edu/pub/papers/google.pdf">The
        Anatomy of a Large-Scale Hypertextual Web Search Engine</a>" by S.
        Brin and L. Page (this link is to the long version, the short version
        was publishied in WWW1998)</p>

        <p>Commentary: "This paper (and the work it reports) has had more
        impact on everyday life than any other in the IR area. A major
        contribution of the paper is the recognition that some relevant search
        results are greatly more valued by searchers than others. By reflecting
        this in their evaluation procedures, Brin and Page were able to see the
        true value of web-specific methods like anchor text. The paper presents
        a highly efficient, scalable implementation of a ranking method which
        now delivers very high quality results to a billion people over
        billions of pages at about 6,000 queries per second. It also hints at
        the technology which Google users now take for granted: spam rejection,
        high speed query-based summaries, source clustering, and
        context(location)-sensitive search. IR and bibliometrics researchers
        had done it all (relevance, proximity, link analysis, efficiency,
        scalability, summarization, evaluation) before 1998 but this paper
        showed how to make it work on the web. For any non-IR engineer
        attempting to build a web-based retrieval system from scratch, this
        must be the first port of call."</p>

      </td>
      <td rowspan="2">Summaries
     </td>
      <td><a href="slides/lecture17-linkanalysis.ppt">Slides</a></td>
    </tr>
    <tr>
      <td>2/28</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td rowspan="2">9</td>
      <td>3/5</td>
      <td rowspan="2">Matrix decompositions and latent semantic indexing</td>
      <td rowspan="2">Textbook <a
        href="http://nlp.stanford.edu/IR-book/information-retrieval-book.html">Chapter
        18</a> : Matrix Decompositions and latent semantic indexing 

	<p><a
	href="http://www.puffinwarellc.com/index.php/news-and-articles/articles/33-latent-semantic-analysis-tutorial.html">
	Additional tutorial on LSA, with code</a>

        <p>11. "<a
        href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.49.7546">Indexing
        by latent semantic analysis</a>" by (Deerwester, Dumais, et.al)</p>

        <p>Commentary: " IR, as a field, hasn't directly considered the issue
        of semantic knowledge representation. The above paper is one of the few
        that does in the following way. LSI is latent semantic analysis (LSA)
        applied to document retrieval. LSA is actually a variant of a growing
        ensemble of cognitively-motivated models referred to by the term
        "semantic space". LSA has an encouraging track record of compatibility
        with human information processing across a variety of information
        processing tasks. LSA seems to capture the meaning of words in a way
        which accords with the representations we carry around in our heads.
        Finally, the above paper is often cited and interest in LSI seems to
        have increased markedly in recent years. The above paper has also made
        an impact outside our field. For example, recent work on latent
        semantic kernels (machine learning) draws heavily on LSI. "</p>

        <p></p>
      </td>
      <td rowspan="2">
	<p>Summaries</p>
	
     </td>
      <td><a href="slides/LSI.pdf">Slides</a></td>
    </tr>
    <tr>
      <td>3/7</td>
      <td>
        &nbsp;
      </td>
    </tr>
    <tr>
      <td rowspan="2">10</td>
      <td>3/12</td>
      <td rowspan="2">Matrix decompositions and latent semantic indexing</td>
      <td rowspan="2">Textbook <a
        href="http://nlp.stanford.edu/IR-book/information-retrieval-book.html">Chapter
        8</a> : Evaluation in Information Retrieval

       <p> 12. "<a
        href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.8829">
	Unsupervised
        Named-Entity Extraction from the Web: An Experimental Study</a> "
        (Etzioni, et.al.)</p>

        <p>Commentary: "This paper represents a new generation of IR work that
        attempts to do more than build a bag of words for information
        retrieval, but also attempts to make some sense of the information as
        well."</p>

        <p></p>
      </td>
      <td rowspan="2">Summaries</td>
      <td>
        <p><a href="slides/OOPSLA08.pdf">Slides</a></p>
        <a href="slides/Lecture19New.pdf">Slides</a>

     </td>
    </tr>
    <tr>
      <td>3/14</td>
      <td><a href="slides/Lecture20.pdf"></a></td>
    </tr>
  </tbody>
</table>

<p></p>

<p><span style="color:#fd4138">Exam: no exam</span></p>

<p>&nbsp;</p>
<hr>

<a name="honesty"><p><b><font size="4">Academic Honesty</font></b></p></a>

<p>I trust all students are honest and do not cheat. Those who break my trust
at any point will get an F in the course - no excuses or apologies will be
accepted.<font size="2" face="Times New Roman, Times"></font><font
face="Times New Roman, Times">Additional penalties may also be imposed by the
department and the university. Very severe incidents of academic dishonesty can
result in suspension or expulsion from the university.</font> </p>

<p>So don't risk it! If, for some reason, you can't do the homework on time or
can't study for the Quiz, you're better off skipping it than cheating it. Do
the math!</p>
<hr>

<a name="disability"><p><b><font size="4">Students with Disability</font></b></p></a>

<p>Any student who feels he or she may need an accommodation based on the
impact of a disability should contact me privately to discuss his or her
specific needs. Also contact the Disability Services Center at (949) 824-7494
as soon as possible to better ensure that such accommodations are implemented
in a timely fashion.</p>
</body>
</html>
