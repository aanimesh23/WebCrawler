<!DOCTYPE html 
    PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" 
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
  <title>SLI | Classes-CS178-Notes / DecisionTrees </title>
  <meta http-equiv='Content-Style-Type' content='text/css' />
  <link rel='stylesheet' href='http://sli.ics.uci.edu/pmwiki/pub/skins/custom/pmwiki.css' type='text/css' />
  <!--HTMLHeader--><style type='text/css'><!--
  ul, ol, pre, dl, p { margin-top:0px; margin-bottom:0px; }
  code.escaped { white-space: nowrap; }
  .vspace { margin-top:1.33em; }
  .indent { margin-left:40px; }
  .outdent { margin-left:40px; text-indent:-40px; }
  a.createlinktext { text-decoration:none; border-bottom:1px dotted gray; }
  a.createlink { text-decoration:none; position:relative; top:-0.5em;
    font-weight:bold; font-size:smaller; border-bottom:none; }
  img { border:0px; }
  .editconflict { color:green; 
  font-style:italic; margin-top:1.33em; margin-bottom:1.33em; }

  table.markup { border:2px dotted #ccf; width:90%; }
  td.markup1, td.markup2 { padding-left:10px; padding-right:10px; }
  table.vert td.markup1 { border-bottom:1px solid #ccf; }
  table.horiz td.markup1 { width:23em; border-right:1px solid #ccf; }
  table.markup caption { text-align:left; }
  div.faq p, div.faq pre { margin-left:2em; }
  div.faq p.question { margin:1em 0 0.75em 0; font-weight:bold; }
  div.faqtoc div.faq * { display:none; }
  div.faqtoc div.faq p.question 
    { display:block; font-weight:normal; margin:0.5em 0 0.5em 20px; line-height:normal; }
  div.faqtoc div.faq p.question * { display:inline; }
   
    .frame 
      { border:1px solid #cccccc; padding:4px; background-color:#f9f9f9; }
    .lfloat { float:left; margin-right:0.5em; }
    .rfloat { float:right; margin-left:0.5em; }
a.varlink { text-decoration:none;}

--></style>  <meta name='robots' content='index,follow' />

</head>
<body>
<!--PageHeaderFmt-->
  <div id='wikilogo'><a href='http://sli.ics.uci.edu'><img src='/pmwiki/pub/skins/custom/SLI_white.png'
    alt='SLI' border='0' /></a></div>
  <div id='wikihead'>
  <form action='http://sli.ics.uci.edu'>
    <!-- <span class='headnav'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes/RecentChanges'
      accesskey='c'>Recent Changes</a> -</span> --> 
    <input type='hidden' name='n' value='Classes-CS178-Notes.DecisionTrees' />
    <input type='hidden' name='action' value='search' />
    <!-- <a href='http://sli.ics.uci.edu/Site/Search'>Search</a>: -->
    <input type='text' name='q' value='' class='inputbox searchbox' />
    <input type='submit' class='inputbutton searchbutton'
      value='Search' />
    <a href='http://sli.ics.uci.edu/Site/Search'>(?)</a>
  </form></div>
<!--/PageHeaderFmt-->
  <table id='wikimid' width='100%' cellspacing='0' cellpadding='0'><tr>
<!--PageLeftFmt-->
      <td id='wikileft' valign='top'>
        <ul><li><a class='wikilink' href='http://sli.ics.uci.edu/Classes/Classes'>Classes</a>
</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Group/Group'>Group</a>
</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Projects/Projects'>Research</a>
</li><li><a class='urllink' href='http://www.ics.uci.edu/~ihler/pubs.html' rel='nofollow'>Publications</a>
</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Code/Code'>Code</a>
</li></ul><div class='vspace'></div><hr />
<div class='vspace'></div>
</td>
<!--/PageLeftFmt-->
      <td id='wikibody' valign='top'>
<!--PageActionFmt-->
        <div id='wikicmds'><ul><li class='browse'><a class='wikilink' href='http://sli.ics.uci.edu/Classes-CS178-Notes/DecisionTrees?action=login'>login</a>
</li></ul>
</div>
<!--PageTitleFmt-->
        <div id='wikititle'>
          <div class='pagegroup'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes'>Classes-CS178-Notes</a> /</div>
          <h1 class='pagetitle'>DecisionTrees</h1></div>
<!--PageText-->
<div id='wikitext'>
<h2>Decision Tree Classifiers</h2>
<p>Decision trees are a well-known classifier type for both discrete and continuous-valued features.  One advantage of decision trees is that they produce very interpretable decision rules; they are easy to evaluate "by hand", so that the factors that went into the class decision can be easily stated.
</p>
<p class='vspace'>A decision tree classifier consists of a sequence of "comparison nodes", at which a single feature of the data point is examined.  For a continuous-valued feature, the decision node compares the feature value to a threshold, and depending on whether the value is above or below the threshold, recurses down the decision tree to the left or right.  At some point, this process reaches a "decision node", at which one of the possible class categories is output.
</p>
<p class='vspace'>For discrete-valued features, it does not really make sense to "threshold" a value.  Instead, there are a number of possible options.  The most straightforward is to have one child node per possible feature value.  However, this results in a non-binary tree, possibly with a high branching factor, and can complicate the score function used in learning (see discussion in Duda and Hart).  Another possibility is to keep the binary tree shape, in which case some discrete values are assigned to the "left" and all others to the "right" child.
</p>
<div class='vspace'></div>
<table border='0' cellspacing='3' ><tr ><td  align='left'><a href="http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/DecisionTrees/decTree0.png" class="minilink" ><img class="mini" src="http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/DecisionTrees/th00---decTree0.png.jpg" title="decTree0" alt="decTree0" border="0" /></a></td><td  align='left'><a href="http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/DecisionTrees/decTree1.png" class="minilink" ><img class="mini" src="http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/DecisionTrees/th00---decTree1.png.jpg" title="decTree1" alt="decTree1" border="0" /></a></td><td  align='left'><a href="http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/DecisionTrees/decTree2.png" class="minilink" ><img class="mini" src="http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/DecisionTrees/th00---decTree2.png.jpg" title="decTree2" alt="decTree2" border="0" /></a></td></tr>
<tr ><td  align='center'>Data</td><td  align='center'>First level</td><td  align='center'>2nd level</td></tr>
</table>
<div class='vspace'></div><h2>Learning Decision Trees</h2>
<p>Each comparison node of a decision tree consists of the selected feature index, and a threshold for comparison.  Typically, we determine the values of these parameters by a simple exhaustive search, looping over all possible features and all possible thresholds and evaluating some score function, then picking the parameters that result in the best score.  Note that, although the threshold is a continuous value, there are only a finite number of possible decisions to make on a given training set.  In particular, when the training data are sorted along the feature being considered, any threshold falling between two given data points results in exactly the same rule on the training data, and thus are typically indistinguishable.  We can thus enumerate the number of unique thresholds, and typically pick the mean of the two nearest data points as the value.
</p>
<div class='vspace'></div><h3>Score functions</h3>
<p>The purpose of a score function is to decide how good any particular split is.  You might think that the classification accuracy would make a good score function, since minimizing it is our true goal.  However, as usual, classification accuracy is not particularly well behaved.  It will often focus on selecting "very specialized" rules that try to get one more data point correct, rather than trying to split groups of data in a more holistic way.  Also, among rules that do not get any additional data points correct, it provides no guidance whatsoever.
</p>
<p class='vspace'>One useful score function is based on the entropy of the class values within each subtree.  The empirical entropy, measured in bits, for a data set <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/0e13cf8df27d7fef4cf65de75ec4e440.png" /> is given by
<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/c1cb0b8ee230261318226a440c73738b.png" />
where <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/3ab635fad306915e27ede571cd8b94e8.png" /> is the empirical distribution of the class value <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/b46b1fcb3da0d044b4bfb4f530561c0b.png" />, i.e., the fraction of data in set <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/0e13cf8df27d7fef4cf65de75ec4e440.png" /> that have class <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/b46b1fcb3da0d044b4bfb4f530561c0b.png" />.
</p>
<p class='vspace'>How does entropy help us decide on a partitioning?  We can use entropy to calculate the so-called <em>expected information gain</em>, which is the average reduction in entropy we see when we adopt some data split.  In particular, suppose that we split a data set <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/0e13cf8df27d7fef4cf65de75ec4e440.png" /> into <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/8eec6e8d187170e3b7368fb58ed8ce86.png" /> with <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/6ee07abb71bc142cfbc53a272ea26c42.png" />.  We compute the expected information gain as
<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/f225edfb8421320706f978aaa3d147e1.png" />
</p>
<p class='vspace'>A common alternative to entropy is the so-called <strong>Gini index</strong>, which measures the variance of the class variable, rather than its entropy.  The Gini index equivalents of the above equations are:
<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/32e554e147536ae67693b14cee32433b.png" />
Again, H is at its minimum (zero) when the variable y is deterministic (a single class) within the subset S, and IG measures the gain, or increase in determinism, caused by conditioning on the split into subsets S1, S2.
</p>
<div class='vspace'></div><h3>Complexity Control and Pruning</h3>
<p>The complexity of a decision tree is essentially determined by its depth.  (How many parameters can a binary decision tree of depth d have?)  We may therefore want to control this complexity by reducing the depth.  Common stopping rules include not proceeding past some maximum depth d, or not continuing to split nodes of the tree that have fewer than K training data points associated with them (since we may not trust our ability to learn a general rule based on so few data).
</p>
<p class='vspace'>Reducing complexity may be particularly desirable if we feel that the extra depth did not significantly improve our performance.  It is often hard to tell whether a split will significantly improve performance when the tree is initially being constructed.  For example, it is easy to make examples where one split provides no measurable gain in accuracy or score, but allows the next level's split to have significant gains.  For this reason, one usually constructs the entire tree and then <em>prunes</em>.  Given the full decision tree, we start at the leaves and walk upward, checking whether each parent had an accuracy nearly equal to that given by its children.  If the gain is below some threshold, we prune the children and continue upward; if not, we cease recursing for this node or its ancestors.
</p>
<div class='vspace'></div><h2>Decision Stumps</h2>
<p>A decision <em>stump</em> is a single-layer decision tree, i.e., a threshold value applied to a single feature.  Although an extremely weak learner (it can only represent extremely simple decision boundaries), it is commonly used in techniques that leverage many weak learners to create a single more powerful learner, such as ensemble methods.
</p>
<div class='vspace'></div><h2>Related links and more information</h2>
<ul><li><a class='urllink' href='http://en.wikipedia.org/wiki/Decision_tree_learning' rel='nofollow'>Learning Decision Trees</a> on wikipedia
</li><li><a class='urllink' href='http://en.wikipedia.org/wiki/ID3_algorithm' rel='nofollow'>ID3 algorithm</a> on wikipedia
</li></ul>
</div>

      </td>
    </tr></table>
<!--PageFooterFmt-->
  <div id='wikifoot'>
    <div class='footnav' style='float:left'> Last modified February 23, 2012, at 12:44 PM</div>
    <div class='footnav' style='float:right; text-align:right'>
    <a href="http://www.ics.uci.edu">Bren School of Information and Computer Science</a><br>
    <a href="http://www.uci.edu">University of California, Irvine</a>
    </div>
  </div>
<!--HTMLFooter--><script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(["_setAccount", "UA-24148957-2"]);
	_gaq.push(["_trackPageview"]);
	(function() {
	  var ga = document.createElement("script"); ga.type = "text/javascript"; ga.async = true;
	  ga.src = ("https:" == document.location.protocol ? "https://ssl" : "http://www") + ".google-analytics.com/ga.js";
	  var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(ga, s);
	  })();
</script>
</body>
</html>
