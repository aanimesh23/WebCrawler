<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-59568806-1', 'auto');
  ga('send', 'pageview');

</script>

<!--
Design by Free CSS Templates
http://www.freecsstemplates.org
Released for free under a Creative Commons Attribution 2.5 License

Name       : Variant Color  
Description: A two-column, fixed-width design with dark color scheme.
Version    : 1.0
Released   : 20120307

-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title> James Steven Supančič III </title>
<link href="http://fonts.googleapis.com/css?family=Arvo" rel="stylesheet" type="text/css" />
<link href="http://fonts.googleapis.com/css?family=Coda:400,800" rel="stylesheet" type="text/css" />
<link href="style.css" rel="stylesheet" type="text/css" media="screen" />
<!-- syntax highlighting -->
<link href="syntaxhighlighter_3.0.83/styles/shCore.css" rel="stylesheet" type="text/css" />
<link href="syntaxhighlighter_3.0.83/styles/shThemeDefault.css" rel="stylesheet" type="text/css" />


</head>
<body>

<div id="header-wrapper">
	<div id="header">
		<div id="logo">
			<h1><a href="#">James Steven Supančič III </a></h1>
			<p> Computer Vision PhD Student at UC Irvine </p>
		</div>
	</div>
</div>
<div id="wrapper">
	<!-- end #header -->
	<div id="page">
		<div id="page-bgtop">
			<div id="page-bgbtm">
				<div id="content">

    <div class="post">
      <a name="HandsICCV"> </a>
      <h2 class="title"> <a href="#"> Depth-based hand pose estimation </a> </h2>
      <p class="meta"><span class="date"> October
	  2015</span><span class="posted">Posted
	  by <a href="#">James</a></span></p>
      <div style="clear: both;">&nbsp;</div>
      <div class="entry">
	<img src="images/data_methods_iccv.png" width="100%"
	     alt="splash for paper"> </td>
      </div>
      
      <ul>
	<li> Our <a href="0317.pdf"> survey and evaluation of hand
	datasets and pose estimation methods </a> will be a poster at
	<a href="http://pamitc.org/iccv15"> ICCV 2015 </a>.
	<li>
	  <td class="pub"> <p>
	      J. Supancic, G. Rogez, Y. Yang, D. Ramanan, J. Shotton. 
	      <b> "Depth-based hand pose estimation: data, methods,
	      and challenges" </b> <i> International Conference on
	      Computer Vision</i> (ICCV), Santiago, Chile, December 2015.
	    </li>
	    <li>
	      Dataset and code <a href="#HandData"> here </a>
	    </li>
	  </ul>
	</div>
	
	<div class="post">
	  <a name="HandsInAction"> </a>
	  <h2 class="title"> <a href="#"> Everyday Hands in Action </a> </h2>
	  <p class="meta"><span class="date"> October 
	      2015</span><span class="posted">Posted
	      by <a href="#">James</a></span></p>
	  <div style="clear: both;">&nbsp;</div>
	  <div class="entry">
	    <img src="images/hands_in_action.png" width="100%"
		 alt="splash for paper"> </td>
				      </div>
	  
	  <ul>
	    <li> We present
	    our <a href="http://www.researchgate.net/publication/282362901_Understanding_Everyday_Hands_in_Action_from_RGB-D_Images">
	    paper </a> on everyday hands in action.
	    <li>
	      <td class="pub"> <p>
		  G. Rogez, J. Supancic,
		  D. Ramanan.
		  <b>"Understanding Everyday Hands in Action from
		  RGB-D Images"</b> <i> International Conference on
		  Computer Vision </i> (ICCV), Santiago, Chile,
		  December 2015.
		</li>
		
	      </ul>
	    </div>
	
				  <div class="post">
				    <a name="HandEgo"> </a>
				    <h2 class="title"> <a href="#"> Pose in Egocentric Workspaces </a> </h2>
				      <p class="meta"><span class="date">June
					  28th, 2015</span><span class="posted">Posted
					  by <a href="#">James</a></span></p>
				      <div style="clear: both;">&nbsp;</div>
				      <div class="entry">
					<img src="images/ego-workspace-splash.png" width="100%"
					     alt="splash for paper"> </td>
				      </div>

				      <ul>
					<li> We present our <a href="https://www.researchgate.net/publication/269041120_Egocentric_Pose_Recognition_in_Four_Lines_of_Code"> paper </a> on hand pose recognition in egocentric workspaces.
					<li>
					  <td class="pub"> <p>
					  G. Rogez, J. Supancic,
					  D. Ramanan.
					  <b>"First-Person Pose Recognition using Egocentric Workspaces"</b> <i>Computer Vision and Pattern Recognition</i> (CVPR), Boston, MA, June 2015.
					    </li>
					    
					    <li>
					      The title was
					      formerly <b> "Egocentric
					      Pose Recognition in Four
					      Lines of Code". </b>
					    </li>
					  </ul>
					</div>
				  
				  <div class="post">
				    <a name="HandData"> </a>
				    <h2 class="title"> <a href="#"> Hand Datasets and Methods </a> </h2>
				      <p class="meta"><span class="date">
					  September
					  29th, 2015</span><span class="posted">Posted
					  by <a href="#">James</a></span></p>
				      <div style="clear: both;">&nbsp;</div>
				      <div class="entry">
					<img src="images/data_methods_splash.png" width="100%"
					     alt="data and eval splash"> </td>
					<ol>
					  <li>
					    Our survey and evaluation of hand datasets and pose estimation methods is
					    <a href="http://arxiv.org/abs/1504.06378"> on arXiv </a>
					  </li>
					  <li>
					    J. Supancic, G. Rogez, Y. Yang, J. Shotton, D. Ramanan. <b>"Depth-based hand pose estimation: methods, data, and challenges
"</b> <i>arXiv preprint arXiv:1504.06378</i> 2015.
					  </li>
					  <li>
					    <ul>
					      <li> <a href="https://github.com/jsupancic/deep_hand_pose"> Deep Convolutional
					      Network w/ Prior
					      Bottleneck source code </a> and a
					      <a href="network_iter_15000.caffemodel"> pre-trained
					      network </a>  </li>
					      <li> <a href="https://github.com/jsupancic/AStar_Dual_Tree_HandPose"
					      > Other methods,
					      specifically including Dual-Tree
					      accelerated 1-NN </a> </li>
					      <li>
						<a href="https://github.com/jsupancic/libhand-public"> Modified
						  LibHand w/ Inverse-Kinematic Annotation </a>
						<iframe width="420" height="315" src="https://www.youtube.com/embed/N8z_oaW4l1Y" frameborder="0" allowfullscreen></iframe>
					      </li>
					      <li>
						<a href="http://wildhog.ics.uci.edu:9090/full.html"> Dataset
						Link </a>
					      </li>
					      </ul>
					  </li>					      
						
					  <li> Links to datasets mentioned in the paper: 
					  <a href="http://robocoffee.org/datasets/" target="_blank">KTH </a>,
					  <a href="http://cvrr.ucsd.edu/LISA/datasets.html">
					      LISA </a>,
					   <a href="http://hpes.bii.a-star.edu.sg/"
						  target="_blank">
					      ASTAR </a>,
					  		<a href="http://research.microsoft.com/en-us/um/people/yichenw/handtracking/"
							   target="_blank">
					      MSR </a>,
					   <a href="http://cims.nyu.edu/~tompson/NYU_Hand_Pose_Dataset.htm"
						  target="_blank">
					      NYU </a>, 
					   <a href="http://www.iis.ee.ic.ac.uk/~dtang/hand.html"
						  target="_blank">
					      ICL </a>, 
					   <a href="http://cvrlcode.ics.forth.gr/handtracking/"
						  target="_blank">
					      FORTH </a>, 
					   <a href="http://handtracker.mpi-inf.mpg.de/projects/handtracker_iccv2013/"
				      target="_blank">
					      Dexter </a>, 
					   <a href="http://cvg.ethz.ch/research/ih-mocap"> ETH-Z </a>,
					   Max-Planck-Gesellschaft (
					      
						<a href="http://files.is.tue.mpg.de/dtzionas/GCPR_2013">
						  Synthetic </a>,
					      
					      
						<a href="http://files.is.tue.mpg.de/dtzionas/GCPR_2014">
						Real </a>), <a href="http://www.cs.technion.ac.il/~twerd/HandNet/">
						  HandNet </a>,
						<a href="http://research.microsoft.com/en-us/downloads/bc02a9d1-4963-4ef6-a508-287e48dc5b32/default.aspx">
						FingerPaint </a>
					      
					  </li>
					  </ol>
				      </div>
				    </div>
				  
				  
				  <div class="post">
				    <h2 class="title"><a href="http://vision.ics.uci.edu/HANDS-2015">
					HANDS-2015 Workshop </h2>
				    <td valign="center" align =
				    "right"> <img src="HANDS-2015/images.png" width="100%"
						  alt="headshot"> </td>
				      </a>
				      <p class="meta"><span class="date">July
					  30th, 2014</span><span class="posted">Posted
					  by <a href="#">James</a></span></p>
				      <div style="clear: both;">&nbsp;</div>
				      <div class="entry">
					I'm organizing a workshop and
					challange associated with
					CVPR-2015. More information
					can be hound <a href="http://vision.ics.uci.edu/HANDS-2015"> here </a>.
				      </div>
				    </div>
				    <
				  <div class="post">
				    <h2 class="title"><a href="#"> 3D
				    Hand Pose in Egocentric RGB+D
				     </a></h2>
				    <td valign="center" align =
				    "right"> <img src="ego_splash.png"
				    alt="egocentric splash figure" width=640 > </td>
				      <p class="meta"><span class="date">Jan
				      5th, 2015</span><span class="posted">Posted by <a href="#">James</a></span></p>
				      <div style="clear: both;">&nbsp;</div>
				      <div class="entry">
					Hand pose estimation from an
					egocentric view using random
					cascades with synthetic depth
					data

					<ul>
					  <li>
					    <p> G. Rogez, M. Khademi,
					    J. Supancic, J. Montiel,
					    D. Ramanan. <b>"3D Hand
					    Pose Detection in
					    Egocentric RGB-D
					    Images"</b> <i> Workshop
					    on Consumer Depth Cameras
					    for Computer Vision,
					    European Conference on
					    Computer Vision</i>
					    (ECCV), Zurich,
					      Switzerland, Sept. 2014. <a href="egocentric_depth_workshop.pdf">PDF</a> </p>
					    </ul>
					    </div>
				    </div>
				  

				      <div class="post" >
					<a name="SPLTT"> </a>
						<h2 class="title"><a href="#"> Self Paced Long Term Tracking </a></h2>
						<td valign="center" align = "right"> <img src="LearningSplashPanda.svg" alt="learning_splash_figure" width=320 height=320> </td>
						<p class="meta"><span class="date">April, 2013</span><span class="posted">Posted by <a href="#">James</a></span></p>
						<div style="clear: both;">&nbsp;</div>
						<div class="entry">
						  Our paper, "Self Paced Learning for Long-Term Tracking" was accepted for publication at CVPR 2013. The paper presents a novel technique 
						  for adapting an appearance model for long term tracking. 
						  <ul>
						    <li> <a href="https://github.com/jsupancic/SPLTT-Release"> Project Source Code (Release Version, WIP) </a> 
						      <ol>
							<li> See it's README.txt for instructions. </li>
							<li> Feel free to email me if you have problems. </li>
						      </ol>
						    </li>
						    <li> <a href="SPLTT.pdf"> Self-paced learning for long-term tracking (Paper in PDF form) </a> </li>
						    <li> <a href="video-data.zip"> Videos and Ground Truth (Data Set) </a> </li>
						    <li> Output Tracks (CSV: x1, y1, x2, y2, cost). If x1 == NaN then the tracker said the target was occluded or out of frame.
						    <ol> 
						      <li> <a href="SPLTT/SPLTT_Tracks_Offline.zip"> Offline Output Tracks (CSV format) </a> </li>
						      <li> <a href="SPLTT/SPLTT_Tracks_Online.zip"> Online Output Tracks  (CSV format) </a> </li>
						    </ol>

						    </li>
						    <li>
						      <td class="pub"> <p> J. Supancic, D. Ramanan. <b>"Self-Paced Learning for Long-Term Tracking"</b> <i>Computer Vision and Pattern Recognition</i> (CVPR), Portland, OR, June 2013.
						    </li>
						  </ul>
						</div>
					</div>

					<div class="post">
						<h2 class="title"><a href="#"> Displaying Real Valued Data in OpenCV </a></h2>
						<p class="meta"><span class="date">Dec 19th, 2012</span><span class="posted">Posted by <a href="#">James</a></span></p>
						<div style="clear: both;">&nbsp;</div>
						<div class="entry">
						  The imageeq function, which follows, equalizes a floating point image before displaying it. It is much better than MATLAB's imagesc for visualizing depth
						  data. 
<pre class='brush: cpp'>
void imageeq(const char* winName, cv::Mat_< float > im)
{
    // compute the order statistics
    vector< float > values;
    for(int rIter = 0; rIter < im.rows; rIter++)
        for(int cIter = 0; cIter < im.cols; cIter++)
                values.push_back(im.at< float >(rIter,cIter));
    std::sort(values.begin(),values.end());
    auto newEnd = std::unique(values.begin(),values.end());
    values.erase(newEnd,values.end());
    
    // compute an equalized image
    Mat showMe(im.rows,im.cols,DataType< uchar >::type);
    float oldQuant = 0;
    for(int qIter = 1; qIter <= 256; qIter++)
    {
        float quantile = static_cast< float >(qIter)/256;
      
        float thresh_low = values[oldQuant*(values.size() - 1)];
        float thresh_high = values[quantile*(values.size() - 1)];
        //printf("q = %f low = %f high = %f\n",quantile,thresh_low,thresh_high);
        for(int rIter = 0; rIter < im.rows; rIter++)
            for(int cIter = 0; cIter < im.cols; cIter++)
            {
                float curValue = im.at< float >(rIter,cIter);
                if(curValue <= thresh_high && curValue >= thresh_low)
                    showMe.at< uchar >(rIter,cIter) = qIter-1;
            }
        
        oldQuant = quantile;
    }
    
    imshow(winName,showMe);
}
</pre>
						</div>
					</div>

					<div class="post">
						<h2 class="title"><a href="#"> Welcome to my academic homepage </a></h2>
						<td valign="center" align = "right"> <img src="JSS3_IHOG.jpe" alt="headshot"> </td>
						<p class="meta"><span class="date">Dec 18th, 2012</span><span class="posted">Posted by <a href="#">James</a></span></p>
						<div style="clear: both;">&nbsp;</div>
						<div class="entry">
						  The above figure contains the <a href="http://mit.edu/vondrick/ihog/"> inverse HOG visualization </a> of the HOG features in a picture of me. I am a first year PhD student at UC Irvine. 
						  I work in computer vision. I'm interested in how we might exploit temporal data (tracking) and depth data (e.g. Kinect) in semi-supervised learning
						  to create effective detectors. 
						</div>
					</div>
				</div>
				<!-- end #content -->
				<div id="sidebar">
					<ul>

						<li>
							<h2> James Steven Supančič III </h2>
							<p> 
							  <br> <image src="JSS3.png" height=180> </br>
							  <br> 4209 Donald Bren Hall (Office) </br>
							  <br> jsupanci<!-- thwart --> at <!-- thwart -->uci.edu </br>
							  <br>
							  <a href="jsupanci.asc">
							  PGP Key </a>
							  </p>
							   <p> <a href="https://github.com/jsupancic"> github profile <a> </p>
							  <h2> Links </h2>							  
							<p> 
							  <br> <a href="http://www.ics.uci.edu/~dramanan/">
							  My Adviser,
							  Deva
							  Ramanan </a> </br>
							  <br> <a href="http://www.gregrogez.net/">
							  My
							  Supervisor/Collaborator,
							  Gregory Rogez </a> </br>
							  <br> <a href="http://vision.ics.uci.edu/"> Computational Vision Group </a> </br>
							  <br> <a href="http://www.ics.uci.edu/"> Department of Computer Science</a> </br>
							  <br> <a href="http://www.uci.edu/"> University of California at Irvine</a> </br>
							</p>
						</li>

					</ul>
				</div>
				<!-- end #sidebar -->
				<div style="clear: both;">&nbsp;</div>
			</div>
		</div>
	</div>
	<!-- end #page -->
</div>
<div id="footer">
	<p>Copyright (c) 2012 James Supancic III. All rights reserved. Design by <a href="http://www.freecsstemplates.org">FCT</a>.</p>
</div>
<!-- end #footer -->
</body>
	<!-- add syntax highlighting -->
	<script src="syntaxhighlighter_3.0.83/scripts/shCore.js"></script>
	<script src="syntaxhighlighter_3.0.83/scripts/shBrushCpp.js"></script>
	<script>
	  SyntaxHighlighter.all()
	</script>

</html>
