<HTML>
<center>
<H3>CompSci 267 Homework set #2</H3>
</center>
<OL>
<LI> Consider the following two codings with source alphabet
     {a,b,c,d,e,f} and code alphabet {0,1}.
     For each code,
   <UL>
   <LI> Is it uniquely decodable?
        If not, prove it by giving two source messages that encode
        to the same code message.
   <LI> If it is uniquely decodable, is it a prefix code?
        If not a prefix code, give an equivalent prefix code with the same
        lengths of code words changing as few codings as possible.
   </UL>
  <TABLE>
  <TR> <TD>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </TD> <TD>a</TD> <TD>b</TD> <TD>c</TD>
       <TD>d</TD> <TD>e</TD> <TD>f</TD>
  <TR> <TD>code 1</TD> <TD>01</TD> <TD>011</TD> <TD>10</TD>
       <TD>1000</TD> <TD>0011</TD> <TD>0111</TD>
  <TR> <TD>code 2</TD> <TD>1010</TD> <TD>001</TD> <TD>101</TD>
       <TD>0001</TD> <TD>1101</TD> <TD>1011</TD>
  </TABLE>
<BR> &nbsp;

<!---
  <LI> In a comma code, one code-symbol is used to end every codeword
     and is used in no other way.  Suppose for <I>a</I>-ary encoding of
     an <I>m</I>-letter alphabet, that the <I>m</I> words consist of the
     <I>m</I> shortest distinct strings of non-comma code letters
     followed by a comma.  Such a code is plainly prefix-free.
     Verify that it satisfies the Kraft inequality.
  </UL>
<BR> &nbsp;
--->

<LI> A full binary tree is <I>right justified</I> if, at each level of
the tree, all the nonterminal vertices appear to the right of all the
leaf vertices.  Show that there must be a lossless encoding method which
employs fewer than 0.85<I>N</I> codebits to encode each right justified
tree having <I>N</I> leaves, if <I>N</I> is sufficiently large.
<BR> &nbsp;
<BR> &nbsp;

<LI> Compact codes come from conservative expansions of a trivial code.
  <UL type=a>
  <LI> Prove that, for each compact code <I>C</I>, there exists exactly
      one expansion sequence from a trivial code to <I>C</I>, in which
      each expansion <I>S</I> &rArr; <I>S'</I>
      is an expansion of either the longest codelength, <I>L</I>,
      or the codelength <I>L</I>-1 in <I>S</I>.
<BR> &nbsp;

   <LI> Write a program to determine
        the number of compact codes of size 40.
   </UL>
<BR> &nbsp;

<!--
<LI> Let the random variable <I>X</I> have five possible outcomes
{1,2,3,4,5}.  Consider two distributions <I>p</I>(<I>x</I>)
and <I>q</I>(<I>x</I>) on <I>X</I>.
<table><tr><td width=25%>&nbsp;</td><td><TABLE border=1 cellspacing=0>
<TR><TD>Symbol</TD>
    <TD><I>p</I>(<I>x</I>)</TD> <TD><I>q</I>(<I>x</I>)</TD>
    <TD>C<sub>1</sub>(<I>x</I>)</TD> <TD>C<sub>2</sub>(<I>x</I>)</TD>
<TR><TD align=center>1</TD>
    <TD>1/2</TD> <TD>1/2</TD> <TD>0</TD> <TD>0</TD>
<TR><TD align=center>2</TD>
    <TD>1/4</TD> <TD>1/8</TD> <TD>10</TD> <TD>100</TD>
<TR><TD align=center>3</TD>
    <TD>1/8</TD> <TD>1/8</TD> <TD>110</TD> <TD>101</TD>
<TR><TD align=center>4</TD>
    <TD>1/16</TD> <TD>1/8</TD> <TD>1110</TD> <TD>110</TD>
<TR><TD align=center>5</TD>
    <TD>1/16</TD> <TD>1/8</TD> <TD>1111</TD> <TD>111</TD>
</TABLE>
</td></table>
  <UL type=a>
  <LI> Calculate H(<I>p</I>), H(<I>q</I>), D(<I>p</I>||<I>q</I>),
       and D(<I>q</I>||<I>p</I>).
  <LI> The last two columns represent codes for the random variable.
      <BR>Verify that the average length of C<sub>1</sub> under <I>p</I>
       is equal to the entropy H(<I>p</I>), and thus C<sub>1</sub>
       is optimal for <I>p</I>.
      <BR>Verify that C<sub>2</sub> is optimal for <I>q</I>.
  <LI> Now assume that we use code C<sub>2</sub> when the distribution
       is <I>p</I>.
      <BR>What is the average length of the codewords, and by how much does
       it exceed the entropy of <I>p</I>.
  <LI> What is the loss if we use code C<sub>1</sub> when the
       distribution is <I>q</I>?
  </UL>
<BR> &nbsp;
-->

<!--
<LI> A deck of <I>n</I> cards in order 1,2,...,<I>n</I> is provided.
     One card is removed at random, then replaced at random.
     What is the entropy of the resulting deck when <I>n</I>=16?
     For general <I>n</I>?
<BR> &nbsp;
-->

<LI> Consider the following generalization of Shannon-Fano coding to
     create a binary codetree for the set S of words {w<sub>i</sub>}
     with associated probabilities {p<sub>i</sub>}:
   <UL type=SQUARE>
   <LI> create root vertex <I>r</I> and associate S with it
   <LI> initialize queue Q to contain <I>r</I>
   <LI> while Q is not empty do
       <UL type=*>
       <LI> dequeue a vertex <I>v</I> having associated set V
       <LI> partition V into two sets V<sub>0</sub> and V<sub>1</sub>
            so that the sum of probabilities in each set is as
            nearly equal as possible
       <LI> create two new vertices, v<sub>0</sub>
            (with set V<sub>0</sub> associated with it) and
            v<sub>1</sub> (with set V<sub>1</sub> associated with it)
       <LI> attach v<sub>0</sub> and v<sub>1</sub>
            as sons of <I>v</I> via edges labeled 0 and 1
       <LI> if |V<sub>0</sub>| > 1 then enqueue v<sub>0</sub>;
            if |V<sub>1</sub>| > 1 then enqueue v<sub>1</sub>
       </UL>
   </UL>
<P>
<UL type=a>
<LI> Examine the cases when set S contains 4 or 5 words to see
     how much the compression rate of the codetree created by
     this approach can exceed entropy.
<LI> Comment on the practicality and effectiveness of this algorithm
     as compared with that of Shannon-Fano.
</UL>
<BR> &nbsp;
<BR> &nbsp;

<LI> Use (a) Shannon's method of coding,
     (b) Shannon-Fano coding,
     and (c) Huffman coding,
     <BR>to determine the average codelength of
     (1) a binary code, and (2) a ternary code,
     <BR>for a source having symbol probability distribution
     <BR>.22, .2, .18, .15, .1, .08, .07.

</OL>
</HTML>
