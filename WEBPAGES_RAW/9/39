<!DOCTYPE html 
    PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" 
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
  <title>SLI | Classes-CS178-Notes / Boosting </title>
  <meta http-equiv='Content-Style-Type' content='text/css' />
  <link rel='stylesheet' href='http://sli.ics.uci.edu/pmwiki/pub/skins/custom/pmwiki.css' type='text/css' />
  <!--HTMLHeader--><style type='text/css'><!--
  ul, ol, pre, dl, p { margin-top:0px; margin-bottom:0px; }
  code.escaped { white-space: nowrap; }
  .vspace { margin-top:1.33em; }
  .indent { margin-left:40px; }
  .outdent { margin-left:40px; text-indent:-40px; }
  a.createlinktext { text-decoration:none; border-bottom:1px dotted gray; }
  a.createlink { text-decoration:none; position:relative; top:-0.5em;
    font-weight:bold; font-size:smaller; border-bottom:none; }
  img { border:0px; }
  .editconflict { color:green; 
  font-style:italic; margin-top:1.33em; margin-bottom:1.33em; }

  table.markup { border:2px dotted #ccf; width:90%; }
  td.markup1, td.markup2 { padding-left:10px; padding-right:10px; }
  table.vert td.markup1 { border-bottom:1px solid #ccf; }
  table.horiz td.markup1 { width:23em; border-right:1px solid #ccf; }
  table.markup caption { text-align:left; }
  div.faq p, div.faq pre { margin-left:2em; }
  div.faq p.question { margin:1em 0 0.75em 0; font-weight:bold; }
  div.faqtoc div.faq * { display:none; }
  div.faqtoc div.faq p.question 
    { display:block; font-weight:normal; margin:0.5em 0 0.5em 20px; line-height:normal; }
  div.faqtoc div.faq p.question * { display:inline; }
   
    .frame 
      { border:1px solid #cccccc; padding:4px; background-color:#f9f9f9; }
    .lfloat { float:left; margin-right:0.5em; }
    .rfloat { float:right; margin-left:0.5em; }
a.varlink { text-decoration:none;}

--></style>  <meta name='robots' content='index,follow' />

</head>
<body>
<!--PageHeaderFmt-->
  <div id='wikilogo'><a href='http://sli.ics.uci.edu'><img src='/pmwiki/pub/skins/custom/SLI_white.png'
    alt='SLI' border='0' /></a></div>
  <div id='wikihead'>
  <form action='http://sli.ics.uci.edu'>
    <!-- <span class='headnav'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes/RecentChanges'
      accesskey='c'>Recent Changes</a> -</span> --> 
    <input type='hidden' name='n' value='Classes-CS178-Notes.Boosting' />
    <input type='hidden' name='action' value='search' />
    <!-- <a href='http://sli.ics.uci.edu/Site/Search'>Search</a>: -->
    <input type='text' name='q' value='' class='inputbox searchbox' />
    <input type='submit' class='inputbutton searchbutton'
      value='Search' />
    <a href='http://sli.ics.uci.edu/Site/Search'>(?)</a>
  </form></div>
<!--/PageHeaderFmt-->
  <table id='wikimid' width='100%' cellspacing='0' cellpadding='0'><tr>
<!--PageLeftFmt-->
      <td id='wikileft' valign='top'>
        <ul><li><a class='wikilink' href='http://sli.ics.uci.edu/Classes/Classes'>Classes</a>
</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Group/Group'>Group</a>
</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Projects/Projects'>Research</a>
</li><li><a class='urllink' href='http://www.ics.uci.edu/~ihler/pubs.html' rel='nofollow'>Publications</a>
</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Code/Code'>Code</a>
</li></ul><div class='vspace'></div><hr />
<div class='vspace'></div>
</td>
<!--/PageLeftFmt-->
      <td id='wikibody' valign='top'>
<!--PageActionFmt-->
        <div id='wikicmds'><ul><li class='browse'><a class='wikilink' href='http://sli.ics.uci.edu/Classes-CS178-Notes/Boosting?action=login'>login</a>
</li></ul>
</div>
<!--PageTitleFmt-->
        <div id='wikititle'>
          <div class='pagegroup'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes'>Classes-CS178-Notes</a> /</div>
          <h1 class='pagetitle'>Boosting</h1></div>
<!--PageText-->
<div id='wikitext'>
<p>Boosting is another, very popular style of ensemble classifier.  Boosted ensembles of learners are trained in a sequential way, with each new learner focusing on the data that are not yet well classified.
</p>
<p class='vspace'>The basic outline of a boosted learner is as follows.  Take any type of base classifier, but make sure that it can train on <em>weighted</em> data.  In other words, rather than attempting to minimize the classification error with our base learner, we will need to minimize a <em>weighted classification error</em>,
<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/7fe3d343d1fc4c3f4a6cc6cf7ac579e9.png" />
Note that if <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/30216a35c011b9754aa2fd325d177e8b.png" /> for all i, this is exactly the standard empirical classification error.
</p>
<p class='vspace'>Now, beginning with uniform weights <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/30216a35c011b9754aa2fd325d177e8b.png" />, train a base classifier <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/85e7a776f882dcb9e07deaeb48111437.png" />.  Predict <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/5a7b7544753a10fe8b2a394aeff2b764.png" /> for each training example.  For any training examples we get right, reduce the weight; for training examples we get wrong, increase it.  Then train the next learner and repeat. This focuses our next learner on those data that we have consistently gotten wrong most often: the "hard" data points.
</p>
<p class='vspace'>Finally, we combine the learners by evaluating them, but rather than a simple average we take a weighted average, multiplying each learner j's decision by a linear coefficient <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/d7646b399ccc83789de5c8f4a3e6252a.png" />.
</p>
<p class='vspace'>The precise details of this process are typically what distinguishes different boosting algorithms.
</p>
<div class='vspace'></div><h2>AdaBoost</h2>
<p>A very popular algorithm is AdaBoost, which was also the boosting algorithm that first popularized the framework.  It is easiest to describe AdaBoost using classes <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/1c280b70e7ee303257be2dd1829defe1.png" />.  
</p>
<p class='vspace'>In AdaBoost, we compute the coefficient for each learner j based on its weighted error rate,
<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/64aa7c44f98af7b4227124b8bcb84005.png" />
We then use this quantity to update the weights as well:
<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/8fde8f0fdc5b5460e45656e53772965a.png" />
and re-normalize the weights to sum to one.  Note that, since <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/1c280b70e7ee303257be2dd1829defe1.png" />, the quantity <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/aced9913283f214ebe2f6955596b5255.png" /> is either +1 or -1 -- it equals +1 if the prediction agrees with the true class, and -1 if it does not.  This up-weights incorrect predictions, and down-weights correct ones.
</p>
<p class='vspace'>The final classifier is given by summing the weighted individual learners and applying a threshold:
<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/e03b9be841e0a071cf115a0925b5af78.png" />
</p>
<div class='vspace'></div><h2>Boosting and Surrogate Cost Functions</h2>
<p>Boosting algorithms can be shown to correspond to a particular surrogate loss function (replacement for the classification error rate).  In particular, AdaBoost corresponds to the exponential loss function
<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/1905b6b75210480fa7ce43307576de89.png" />
Note again that, if y and F(x) share the same sign (the prediction is correct), this value will be small (and smaller the more confident F(x) is).  At the decision boundary, the cost is 1, and continues to increase the more F(x) is confident in its incorrect prediction.  The exponential loss function is nice in part because it is both <em>convex</em> and upper bounds the 0/1 loss (classification error).  The first property makes it easy to optimize, while the second means that as we optimize we are at least trying to push downward on the classification loss below us.
</p>
<div class='vspace'></div>
</div>

      </td>
    </tr></table>
<!--PageFooterFmt-->
  <div id='wikifoot'>
    <div class='footnav' style='float:left'> Last modified February 25, 2012, at 01:44 PM</div>
    <div class='footnav' style='float:right; text-align:right'>
    <a href="http://www.ics.uci.edu">Bren School of Information and Computer Science</a><br>
    <a href="http://www.uci.edu">University of California, Irvine</a>
    </div>
  </div>
<!--HTMLFooter--><script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(["_setAccount", "UA-24148957-2"]);
	_gaq.push(["_trackPageview"]);
	(function() {
	  var ga = document.createElement("script"); ga.type = "text/javascript"; ga.async = true;
	  ga.src = ("https:" == document.location.protocol ? "https://ssl" : "http://www") + ".google-analytics.com/ga.js";
	  var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(ga, s);
	  })();
</script>
</body>
</html>
