
<!-- saved from url=(0032)http://www.ics.uci.edu/~vpsaini/ -->
<html><head><meta charset='utf-8'>
<title>The Yelp dataset challenge - Multilabel Classification of Yelp reviews into relevant categories</title>
<meta name="description" content="Yelp dataset challenge">
<meta name="keywords" content="Yelp, challenge, data mining">
<style>
A { COLOR: #62021; TEXT-DECORATION: none ;  }
a:visited{ color: blue }
A:hover { TEXT-DECORATION: underline }
.lowspace { min-height: 5}
.midspace { min-height: 10}
.highspace { min-height: 15}
.media {width:459;height:358;}
.left{float:left;}
.right{float:right;}
.photo{height: 309;
	width: 430;
	padding-left: 57;
	padding-bottom: 25;}
	table {
		padding-left: 20;
		padding-right: 20;
	}
	td {
		text-align: justify;
		font-size: 110%;
		line-height: 150%;
		margin-bottom: 48px;
		margin-top: -22px;
		font-family: Georgia;
	}
	divimg {
		text-align: center;
		font-size: 110%;
		line-height: 150%;
		margin-bottom: 48px;
		margin-top: -22px;
		font-family: Georgia;
	}
	.mondego{width:300px;
		height:110px;
		background: transparent url('http://mondego.ics.uci.edu/img/mondego-banner.png') -0px -0px no-repeat;}
		</style>

		<style type="text/css"></style><style type="text/css"></style></head>
		<body link="blue" alink="blue" vlink="violet">
			<script>
			  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

			  ga('create', 'UA-41064162-1', 'uci.edu');
			  ga('send', 'pageview');

			</script>
						

			<table>
				<tbody><tr><td width="20"><img src="./files/logo.png" width="100" height="100">
					<!--<tr><td width=20><img src="images/shipra-snow.JPG" width=600 height=400></img>-->
				</td><td width="20">
			</td><td> <h2><a href="http://www.yelp.com/dataset_challenge/">Yelp Dataset Challenge</a></h2>

			<i>
				The Team<br>
				<a href ="http://www.ics.uci.edu/~hsajnani/">Hitesh Sajnani</a>, <a href="http://www.linkedin.com/in/sainivaibhav">Vaibhav Saini</a>, <a href ="http://www.linkedin.com/in/kusumkumar">Kusum Kumar</a>
				, <a href ="http://www.linkedin.com/in/egabrielova">Eugenia Gabrielova </a>, <a href="http://www.linkedin.com/in/pramitc">Pramit Choudary</a>, <a href="http://www.ics.uci.edu/~lopes/">Cristina Lopes</a>  <br>
			</i>
		</div></div></td>
		<td>
			<div class="mondego"></div>
		</td></tr></tbody></table>
		<table>
			<tbody>
				<tr><td><br><h3>Classifying Yelp reviews into relevant categories</h3>
					Yelp users give ratings and write reviews about businesses and services on Yelp. These reviews and ratings help other Yelp users to evaluate a business or a service and make a choice. While ratings are useful to convey the overall experience, they do not convey the context which led a reviewer to that experience. 
					For example, consider a yelp review about a restaurant which has 4 stars: <br>
					<blockquote><i><strong>"They have the best happy hours, the food is good, and service is even better. When it is winter we become regulars".</strong></i></blockquote> <br> 

					If we look at only the rating, it is difficult to guess why the user rated the restaurant as 4 stars. However, after reading the review, it is not difficult to identify that the review talks about good "food", "service" and "deals/discounts" (happy hours). <br> <br>
					
					A quick inspection of few hundred reviews helped us to decide important categories that are frequent in the reviews. We found 5 categories which include âFoodâ, âServiceâ, âAmbienceâ, âDeals/Discountsâ, and âWorthinessâ. "Food" and "Service" categories are easy to interpret. "Ambience" category relates to the dÃ©cor, and look and feel of the place. "Deals and Discounts" category correspond to offers during happy hours, or specials run by the venue. âWorthinessâ category can be summarized as value for money. Users often express the sentiment whether the overall experience was worth the money. It is important to note that "Worthiness" category is different from the âPriceâ attribute already provided by Yelp. "Price" captures whether the venue is âinexpensiveâ, âexpensiveâ or âvery expensiveâ. <br> <br>
					
					This high level categorization of reviews into relevant categories can help user to understand why the reviewer rated the restaurant as âhighâ or âlowâ. This information can help other yelpers to make a personalized choice, especially when one does not have much time to spend on reading the reviews. Moreover, such categorization can also be used to rank restaurants according to these categories.<br><br>
					
					We formulted the task of classifying a review into relevant categories as a learning problem. However, since a review is inexclusively associated with multiple categories at the same time, it is not a simple binary classification or a multi-class classification. It is rather a multi-label classification problem. <br>

					Here is a short video describing how (and why?) Yelp can build some cool features using this categorization: <br> <br> 
					<iframe   class="media" src="http://www.youtube.com/embed/jPQCiSmxwrg" frameborder="0" allowfullscreen></iframe>
					
					
					<!--For example, in case of a restaurant, the rating might be influenced by the food, or the ambience, or the service, or the discounts offered by the restaurant, maybe all or some combination of these. This information is not conceivable from only rating, however, it is present in the reviews which users write. We conjecture that such contextual information can be automatically extracted from the reviews. This contextual information when presented to the user by classifying reviews into various relevant categories can prove to be very effective in making an informed decision. Having such a high level categorization can allow users to make a quick personalized choice when one does not have much time to spend on reading the reviews. Moreover, such information can also be used to rank venues based on the categories.  --><br>
					
		<!--This will help users to choose a venue according to the criteria they value more. For example, a user may give more weightage to food, whereas other may value ambience and service more. Currently, yelp users have no idea if reviewers priority of ambience, food, service, price was same as theirs.  We choose {Food, Service, Ambience, Deals/Discount, and Price} as our categories for classification.  .<br>
	-->
</tr>
<!--
<tr>
	<td><br>
		<h3>1.  Introduction </h3>
		Yelp users give ratings and write reviews about businesses and services on Yelp. These reviews and rating help other Yelp users to evaluate a business or a service and make a choice. While ratings are useful to convey the overall experience, they do not convey the context which led users to that experience. For example, in case of a restaurant, the rating might be influenced by the food, or the ambience, or the service, or the discounts offered by the restaurant, maybe all or some combination of these. This information is not conceivable from only rating, however, it is present in the reviews which users write. We conjecture that such contextual information can be automatically extracted from the reviews. This contextual information when presented to the user by classifying reviews into various relevant categories can prove to be very effective in making an informed decision. Including such high level categorization can allow users to make a quick personalized choice when one does not have much time to spend on reading the reviews. Moreover, such information can also be used to rank venues based on the categories.

		Although, the functionality described above is desirable and useful for any kind of business, we limit the scope of our problem for only restaurants. <br>To understand the problem in this context, consider a Yelp review:

		
		<br>
	</td>
</tr>
-->
<tr>
	<td><br>
		<h3>Corpus </h3>
		The <a href="http://www.yelp.com/dataset_challenge">Yelp dataset</a> released for the academic challenge contains information for 11,537 businesses. This dataset has 8,282 check-in sets, 43,873 users, 229,907 reviews for these businesses. For our study, since we are only interested in the restaurant data, we have considered out only those business that are categorized as food or restaurants. This reduced the number of business to around 5,000. <br> <br>
		We selected all the reviews for these restaurants that had atleast one useful vote. From this pool of useful reviews, we randomly chose 10,000 reviews. A labeling codebook describing what categories to include was developed through an initial open coding of a random sample of 400 reviews. The codebook was validated and reï¬ned based on a second random sample of 200 reviews. This exercise helped us to fix 5 categories which include <i>food, ambience, service, deals, and worthiness</i>. Once we identified these 5 categories, the 10,000 reviews were divided into 5 bins with repitition in each bin. 6 Graduate student researchers from our group then read and annotated each of these reviews in the identified categories. <b>It took us approx. 225 man hours to annotate all the reviews.</b> We identified the conflicts in the annotation of reviews among different annotators. We removed all the reviews from the analysis where there were discrepancies among the annotators. This left us with 9019 reviews. We split these annotated reviews into 80% train and 20% test data. <br><br>

		The review annotation process was very challenging and time consuming. We believe that it is one of the major contributions of this work. We plan to release the annotated data for researchers to extend the work. 
		<!--The annotated train and the test dataset can be downloaded from here: <a href="./files/Train.csv"> Train data</a> and <a href="./files/Test.csv"> Test data</a> -->
		<br>

	</td>
</tr>
<td><br>
	<h3>Feature Extraction and Normalization</h3>
	We extracted two types of features: (i) star ratings and (ii) textual features consisting of unigrams, bigrams and trigrams. <br>
	For star ratings we created three binary features representing rating 1-2 stars, 3 stars, and 4-5 stars respectively. <br>
	For extracting textual features, we ï¬rst normalized the review text by converting it to lowerâcase and removing the special characters. We did not remove the stop words as they play important role to understand user sentiments.
	The cleaned text is then tokenized to collect unigrams (individual words) and calculate their frequencies across the entire corpus. 
	This results in 54,121 unique unigrams. We condense this feature set by only considering unigrams with a frequency greater than 300, which results in <a href="./files/unigrams.txt">375 unigram features</a>. 
	Similarly we extract <a href="./files/bigrams.txt">208 bigrams</a> and <a href="./files/trigrams.txt">120 trigrams</a>.
	<br><br>
	The arff files for the the features extracted for Train and Test data can be downloaded from here: <a href="./files/train.arff"> Train.arff</a> and <a href="./files/test.arff"> Test.arff</a>
	<br> <br>
	Here is a short video describing the corpus and the feature engineering <br> <br> 
	<iframe class= "media" src="http://www.youtube.com/embed/0M-Mz-Uzogs" frameborder="0" allowfullscreen></iframe>
	<br>
</td>
</tr>
<tr>
	<td><br>
		<h3>Classification</h3>
		<div>
		In this section, we will describe the various approaches we took to build a classifier. We will reason about our choices based on the advantage and disadvantage of each approach. 
		You can also have a look at the video presentation, to get a quick overall idea<br> <br> 
		<iframe   class="media" src="http://www.youtube.com/embed/L4hzuaHD8s4" frameborder="0" allowfullscreen></iframe>
		<br>

		The problem of classifying a review into multiple categories is a not a simple binary classification problem. Since a reviewer can talk about various things in his or her review, each review can be classified into multiple categories. <br> <br>
		
		One of the most popular and perhaps the simplest way to deal with multiple categories is to create a binary classifier for each category. So in our case, we create 5 binary classifiers for food, service, ambience, deals, and worthiness category. In order to do this, we need to transform the dataset into 5 different datasets where each dataset has information only about one category. <br> <br>
		
		To understand consider a scaled down version of our dataset which has only 4 categories (food, ambience, service, and deals)
		As shown in Figure 1., we create four different dataset from this original dataset such that each dataset is only associated with a specific category. For example, The new dataset created for "food" category will have only one label. The label will be '1' for all the datapoints which had '1' for "food" in the original dataset. Similarly the dataset created for "service" will have label as '1' for all the datapoints that had service as '1' in the original dataset. This is done for all the categories present in the dataset. 
		
		Given a new review, binary classifier for each category predicts if the review belongs to a category. The final prediction is the union of all the binary predictors. <br> <br>
	 
		<div style="width:500;height:375;text-align:center;"> <img  width="500" height="358" src="./files/binaryrelevance.png" align="middle">Figure 1. </div>
		 
		<br>
	
		Once the dataset is transformed in 4 different datasets, any binary classifier like nearest neighbour, SVM, decision trees, etc. can be used with this approach.  Although this approach is pretty simple and treats each category independently. However, as a consequence, it ignores correlation among categories. This assumption may not hold true, especially if the categories share some aspects with each other. For example, in our case, mostly when people get deals, they feel the restaurant is worthy to visit. This means that "deals" and "worthiness" categories are correlated. Similarly we saw correlation between "service" and "ambience" category. Sometimes the correlation may be high, and sometimes it may be very low. However, in our case, we thought it was worth accounting for. <br> <br>
		
		In order to account for correlation among categories, we considered each different subset of L as a single category. Here L is the set of all the categories, a.k.a targets. So L = {Food, Service, Ambience, Deals}. For example, as shown in Figure 2, we transform the target for the Review 1, {Food, Deals} as a single target with value "1001". Similary for Review 2, {Ambience, Deals}, is tranformed into "0011". Here the pattern is formed by creating a vector which has fixed indices for each category. We then learn a multi-class classifier h : X --> P(L), where X is the review, P(L) is the powerset of L, containing all possible category subsets. This approach takes into account correlations between categories, but also suffers from the large number of category subsets. E.g., if we have 5 categories, this approach will generate 2<sup>5</sup> possible targets to predict; most of which will have only few datapoints to learn. This approach might work well if there is a large training dataset which covers all (at least most of) the possible targets to predict.		
		<br> <br>
		<div style="width:500;height:375;text-align:center;">  
		<img   width="500" height="358"  src="./files/classifier_each_subset.gif" align="middle">Figure 2. 
		</div>
		<br>
		We wanted to get best of both worlds i.e., consider correlation among categories and at the same time not get hit by the large number of subsets generated by the previous approach. Hence, we decided to use ensemble of classifiers where each classifier is trained using a different <b>small subset</b> (k) of categories. 
		For example, let's say there are 4 categories {Food, Service, Ambience, Deals}. We choose subset size = 2. Hence, we build a total of <sup>4</sup>C<sub>2</sub> = 6 classifiers for the following  combination of categories: {(Food,Service), (Food,Ambience), (Food, Deals), (Service, Ambience), (Service, Deals), (Ambience, Deals)}. See first part of Figure 3. 
		For prediction, as shown in second part of Figure 3, we consider prediction of all the six classifiers and then take a majority vote. <br> 

		This approach considers correlations among categories and at the same time does not generate very large number of targets by considering only a small subset of categories for each classifier. <br> <br>
		
		<div style="width:950;height:375;text-align:center;"> 
		<img   class="media"  src="./files/ensemble_subset_classifiers.gif" align="middle"> <img   class="media" src="./files/ensemble_classifiers_prediction.gif" align="middle">
		Figure 3. 
		</div>
		
	</td>
</tr>
<tr>
	<td>
		<br>
		<h3>Experiments</h3>
		
		<h4>Evaluation metrics</h4>
		We use Precision and Recall to measure the performance of a classifier. <br>
		To understand, what precision and recall means in our context,
		consider (x,Y) to be a datapoint where x is the review text and Y is the set of true categories. Y &#8838 L, where L = {Food, Service, Ambience, Deals, Worthiness}. <br>
		Let h be a classifier <br>
		Let Z = h(x) be the set of categories predicted by h for the datapoint(x, Y). Then, <br>
		Precision = |Y &#8745 Z|/|Z|  (Out of the categories predicted, how many of the them are true categories) <br>
		Recall = |Y &#8745 Z|/|Y| (Out of the total true categories, how many of them were predicted)<br>

		<h4>Results </h4>
		
		We experimented with all the three approaches discussed above. We used Precision and Recall as our evaluation metrics. For each review, dThe comprehensive set of experiment configurations (different approaches, different classifiers, different feature sets, paramter settings) can be found in this<a href="https://docs.google.com/spreadsheet/ccc?key=0Ahz5yC6y5YLVdEg5blgyRU5wR2lyYVBJREJLMzlDYlE&usp=sharing"> result sheet</a href>. <br> <br> 
		
		In the first approach of using L binary classifiers, where L is the total number of categories, we used Naive Bayes, k-Nearest Neighbour, Support Vector Machines (SMO implementation), decision trees, and Neural Networks. In the figure below we report results for Naive Bayes and K-NN for this approach as only they were competitive. <br>
		
		In the second approach, where we consider label correlations and predict the powerset of labels. Decision trees performed the best in this category. We also experimented with ensemble of classifiers approach using decision trees that gave us the best results overall. <br> <br>
		
		<div class="left"><img width="400" height="300" class="photo" src="./files/resultstrain.png">
		</div>
		<div class="left">
			<img width="400" height="300" class="photo" src="./files/resulttest.png">
		</div>
	</td>
</tr>


<tr>
	<td><br>
		<h3> Conclusion </h3>
		Yelp	 reviews	 and	 ratings	 are	 important	 source	 of	 information	 to	 make	 informed
decisions	about	a	venue.	We	conjecture	 that	 further	classification	of	yelp	reviews	into
relevant	categories	can	help	users	to	make	an	informed	decision	based	on	their	personal
preferences	for	categories.	Moreover,	this	aspect	is	especially	useful	when	users	do	not
have	 time	 to	 read	 many	 reviews	 to	 infer	 the	 popularity	 of	 venues	 across	 these
categories.
In	 this	 paper,	 we	 demonstrated	 how	 reviews	 for	 restaurants	 can	 be	 automatically
classified	 into	 five	 relevant	 categories	 with	 precision	 and	 recall	 of	 0.72 and	 0.71
respectively.	 We	 found	that	 an ensemble	 of	 two	 multi-label	 classification	 technique
(Binary	 Relevance	 and	 Label	 Powerset)	 performed	 better	 than	 the	 techniques
individually.	Moreover,	 there	is	 no	 significant	 difference	in	 performance	when	 using	a
combination	 of	 bigrams,	 unigrams	 and	 trigrams	 instead	 of	 only	 unigrams.	 We	 also
showed	how	the	results	of	this	study	can	be	incorporated	into	Yelpâs	existing	website.

	</td>
</tr>
<tr>
	<td><br>
		<h3> Technical Report </h3>
		<a href="./files/technical_report.pdf">Multilabel Calssification of reviews in Yelp data</a>

	</td>
</tr>
<tr>
	<td><br>
		<h3> Download Presentation </h3>
		<a href="./files/Yelp_Review_Classification.pptx">Multilabel Calssification of reviews in Yelp data</a>

	</td>
</tr>

<tr>
	<td><br>
		<!--
		<h3> References </h3>
		[1] Zhang, M.-L., and Zhou, Z.-H. 2007. Ml-knn: A lazy learning approach to multi-label learning, pattern recognition. Pattern Recognition 40(7):2038â2048.<br>
		[2] Tsoumakas, G.; Vilcek, J.; Spyromitros, E.; and Vlahavas, I. 2010. Mulan: A java library for multi-label learning. Journal of Machine Learning Research.<br>
		[3]Tsoumakas, G., Katakis, I., Vlahavas, I. (2010) "Mining Multi-label Data", Data Mining and Knowledge Discovery Handbook, O. Maimon, L. Rokach (Ed.), Springer, 2nd edition, 2010.<br>
		-->
	</td>
</tr>
</tbody>


</table>
        <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'yelpdatasetchallenge'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

</body></html>
