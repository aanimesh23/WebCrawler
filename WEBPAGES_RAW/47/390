



<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">
<html>
<head>
<title>UCI Machine Learning Repository: Human Activity Recognition Using Smartphones Data Set</title>

<!-- Stylesheet link -->
<link rel="stylesheet" type="text/css" href="../assets/ml.css" />

<script language="JavaScript" type="text/javascript">
<!--
function checkform ( form )
{
  // see http://www.thesitewizard.com/archive/validation.shtml
  // for an explanation of this script and how to use it on your
  // own website

  // ** START **
  if (form.q.value == "")
  {
    alert( "Please enter search terms." );
    form.q.focus();
    return false ;
  }

  if (getCheckedValue(form.sitesearch) == "ics.uci.edu" && form.q.value.indexOf("site:archive.ics.uci.edu/ml") == -1)
  {
    form.q.value = form.q.value + " site:archive.ics.uci.edu/ml";
  }

  // ** END **
  return true ;
}

// return the value of the radio button that is checked
// return an empty string if none are checked, or
// there are no radio buttons
function getCheckedValue(radioObj) {
	if(!radioObj)
		return "";
	var radioLength = radioObj.length;
	if(radioLength == undefined)
		if(radioObj.checked)
			return radioObj.value;
		else
			return "";
	for(var i = 0; i < radioLength; i++) {
		if(radioObj[i].checked) {
			return radioObj[i].value;
		}
	}
	return "";
}
//-->
</script>

</head>

<body>


<!-- SITE HEADER (INCLUDES LOGO AND SEARCH BOX) -->

<table width=100% bgcolor="#003366">
<tr>
	<td>
		<span class="normal"><a href="../index.html" alt="Home"><img src="../assets/logo.gif" 
border=0></img></a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://cml.ics.uci.edu"><font color="FFDD33">Center for Machine Learning and Intelligent Systems</font></a></span>
	</td>
	<td width=100% valign=top align="right">
		<span class="whitetext">
		<a href="../about.html">About</a>&nbsp;
		<a href="../citation_policy.html">Citation Policy</a>&nbsp;
		<a href="../donation_policy.html">Donate a Data Set</a>&nbsp;
		<a href="../contact.html">Contact</a>
		</span>

		<br>
		<br>
		<!-- Search Google -->

		<FORM method=GET action=http://www.google.com/custom onsubmit="return checkform(this);">
		<INPUT TYPE=text name=q size=30 maxlength=255 value="">
		<INPUT type=submit name=sa VALUE="Search">
		<INPUT type=hidden name=cof VALUE="AH:center;LH:130;L:http://archive.ics.uci.edu/assets/logo.gif;LW:384;AWFID:869c0b2eaa8d518e;">
		<input type=hidden name=domains value="ics.uci.edu">
		<br>
		<input type=radio name=sitesearch value="ics.uci.edu" checked> <span class="whitetext"><font size="1">Repository</font></span>
		<input type=radio name=sitesearch value=""> <span class="whitetext"><font size="1">Web</font></span>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		<A HREF=http://www.google.com/search><IMG SRC=http://www.google.com/logos/Logo_25blk.gif border=0 ALT=Google align=middle height=27></A>
		<br>
		</FORM>
		<!-- Search Google -->


		<span class="whitetext"><a href="../datasets.html"><font size="3" color="#FFDD33"><b>View 
ALL Data Sets</b></font></a></span>
		<br>
	</td>
</tr>
</table>


<br />
<table width=100% border=0 cellpadding=2><tr><td>

   <table><tr>
     <td valign=top>
	<p>
	<span class="heading"><b>Human Activity Recognition Using Smartphones Data Set</b></span>
	<br><span class="normal"><i><font size=4 >Download</font></i>: <a href="../machine-learning-databases/00240/"><font 
style="BACKGROUND-COLOR: #FFFFAA" size=4>Data Folder</font></a>, <a href="#"><font 
style="BACKGROUND-COLOR: #FFFFAA" size=4>Data Set Description</font></a></span></p>

	<p class="normal"><b>Abstract</b>: Human Activity Recognition database built from the recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors.</p>
     </td>
     <td> </td>
   </tr></table>

<table border=1 cellpadding=6>
	<tr>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Data Set Characteristics:&nbsp;&nbsp;</b></p></td>
		<td><p class="normal">Multivariate, Time-Series</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Number of Instances:</b></p></td>
		<td><p class="normal">10299</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Area:</b></p></td>
		<td><p class="normal">Computer</p></td>
	</tr>

	<tr>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Attribute Characteristics:</b></p></td>
		<td><p class="normal">N/A</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Number of Attributes:</b></p></td>
		<td><p class="normal">561</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Date Donated</b></p></td>
		<td><p class="normal">2012-12-10</p></td>
	</tr>
	<tr>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Associated Tasks:</b></p></td>
		<td><p class="normal">Classification, Clustering</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Missing Values?</b></p></td>
		<td><p class="normal">N/A</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Number of Web Hits:</b></p></td>
		<td><p class="normal">447085</p></td>
	</tr>
	<!--
	<tr>
		
		<td bgcolor="#DDEEFF"><p class="normal"><b>Highest Percentage Achieved:&nbsp;&nbsp;</b></p></td>
		<td><p class="normal">N/A</p></td>
	</tr>
	-->
</table>


<br />

<p class="small-heading"><b>Source:</b></p>
<p class="normal">Jorge L. Reyes-Ortiz(1,2), Davide Anguita(1), Alessandro Ghio(1), Luca Oneto(1) and Xavier Parra(2)<br>1 - Smartlab - Non-Linear Complex Systems Laboratory<br>DITEN - Università  degli Studi di Genova, Genoa (I-16145), Italy. <br>2 - CETpD - Technical Research Centre for Dependency Care and Autonomous Living<br>Universitat Politècnica de Catalunya (BarcelonaTech). Vilanova i la Geltrú (08800), Spain<br>activityrecognition '@' smartlab.ws </p>

<br />

<p class="small-heading"><b>Data Set Information:</b></p>
<p class="normal">The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. <br><br>The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.<br><br>Check the README.txt file for further details about this dataset. <br><br>A video of the experiment including an example of the 6 recorded activities with one of the participants can be seen in the following link: <a href="http://www.youtube.com/watch?v=XOEN9W05_4A">[Web Link]</a><br><br>An updated version of this dataset can be found at <a href="http://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions">[Web Link]</a>. It includes labels of postural transitions between activities and also the full raw inertial signals instead of the ones pre-processed into windows. </p>

<br />

<p class="small-heading"><b>Attribute Information:</b></p>
<p class="normal">For each record in the dataset it is provided:<br>- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.<br>- Triaxial Angular velocity from the gyroscope. <br>- A 561-feature vector with time and frequency domain variables. <br>- Its activity label. <br>- An identifier of the subject who carried out the experiment. </p>

<br />

<p class="small-heading"><b>Relevant Papers:</b></p>
<p class="normal">Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012  <br><br>Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge L. Reyes-Ortiz.  Energy Efficient Smartphone-Based Activity Recognition using Fixed-Point Arithmetic. Journal of Universal Computer Science. Special Issue in Ambient Assisted Living: Home Care.   Volume 19, Issue 9. May 2013<br><br>Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. 4th International Workshop of Ambient Assited Living, IWAAL 2012, Vitoria-Gasteiz, Spain, December 3-5, 2012. Proceedings. Lecture Notes in Computer Science 2012, pp 216-223. <br><br>Jorge Luis Reyes-Ortiz, Alessandro Ghio, Xavier Parra-Llanas, Davide Anguita, Joan Cabestany, Andreu Català. Human Activity and Motion Disorder Recognition: Towards Smarter Interactive Cognitive Environments. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.  </p>

<br />


<!-- OLD CODE:

<p class="small-heading"><b>Papers That Cite This Data Set<sup>1</sup>:</b></p>
<img src="../assets/rexa.jpg" />
<p class="normal">N/A</p>

-->



<br />

<p class="small-heading"><b>Citation Request:</b></p>
<p class="normal">Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013. </p>

</td></tr></table>


<hr>

<!-- OLD CODE:
<p class="normal"><font size=1>[1] Papers were automatically harvested and associated with this data set, in collaboration with <a href="http://rexa.info"><font size=1>Rexa.info</font></a></font></p>
-->



<table cellpadding=5 align=center><tr valign=center>
		<td><p class="normal">Supported By:</p></td>
        <td><img src="../assets/nsfe.gif" height=60 /> </td>
        <td><p class="normal">&nbsp;In Collaboration With:</p></td>
        <td><img src="../assets/rexaSmall.jpg" /></td>
</tr></table>

<center>
<span class="normal">
<a href="../about.html">About</a>&nbsp;&nbsp;||&nbsp;
<a href="../citation_policy.html">Citation Policy</a>&nbsp;&nbsp;||&nbsp;
<a href="../donation_policy.html">Donation Policy</a>&nbsp;&nbsp;||&nbsp;
<a href="../contact.html">Contact</a>&nbsp;&nbsp;||&nbsp;
<a href="http://cml.ics.uci.edu">CML</a>
</span>
</center>




</body>
</html>
