
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Computational Vision | ICS | UC Irvine</title>
  <link rel="stylesheet" type="text/css" href="/stylesheets/screen.css" media="all">
</head>

<body id="publications">
  <div id="outerframe">
  <div id="header">
    <h1>Computational Vision at UC Irvine &nbsp;<img src="/images/eye_small.jpg" alt="small eye"></h1>
  </div>
  <div id="tabs">
    <ul id="tabnav">
      <li id="home_tab"><a href="/index.html">home</a></li>
      <li id="projects_tab"><a href="/projects.html">projects</a></li>
      <li id="people_tab"><a href="/people.html">people</a></li>
      <li id="publications_tab"><a href="/publications.html">publications</a></li>
      <li id="datasets_tab"><a href="/datasets/index.html">datasets</a></li>
      <li id="events_tab"><a href="/events.html">events</a></li>
      <li id="courses_tab"><a href="/courses.html">courses</a></li>
      <li id="contact_tab"><a href="/contact.html">contact</a></li>
      <li id="links_tab"><a href="/links.html">links</a></li>
    </ul>
  </div>

  <div id="content">
    <div id="paper_title">Video Annotation and Tracking with Active Learning</div>
    <div id="paper_authors"><a href="/people/26.html">Carl&nbsp;Vondrick</a>, <a href="/people/20.html">Deva&nbsp;Ramanan</a></div>
    <div id="abstract">
      <img style="float:left;" src="icon_drop.jpg" alt="icon">
      We introduce a novel active learning framework for video annotation. By 
judiciously choosing which frames a user should annotate, we can obtain highly
accurate tracks with minimal user effort. We cast this problem as one of active
learning, and show that we can obtain excellent performance by querying frames
that, if annotated, would produce a large expected change in the estimated object
track. We implement a constrained tracker and compute the expected change for
putative annotations with efficient dynamic programming algorithms. We demonstrate
our framework on four datasets, including two benchmark datasets constructed with
key frame annotations obtained by Amazon Mechanical Turk. Our results indicate
that we could obtain equivalent labels for a small fraction of the original cost.

    </div>
    <div id="bibtext">
      <h3>Download: <a href="/papers/VondrickR_NIPS_2011/VondrickR_NIPS_2011.pdf">pdf</a></h3>
      <h3>Text Reference</h3>
Carl Vondrick and Deva Ramanan.
Video annotation and tracking with active learning.
In <em>NIPS</em>, 28&ndash;36. 2011.<br>
<h3>BibTeX Reference</h3>
@inproceedings{VondrickR_NIPS_2011,<br>
&nbsp;&nbsp;&nbsp;&nbsp;author = "Vondrick, Carl and Ramanan, Deva",<br>
&nbsp;&nbsp;&nbsp;&nbsp;title = "Video Annotation and Tracking with Active Learning",<br>
&nbsp;&nbsp;&nbsp;&nbsp;booktitle = "NIPS",<br>
&nbsp;&nbsp;&nbsp;&nbsp;year = "2011",<br>
&nbsp;&nbsp;&nbsp;&nbsp;pages = "28-36"<br>
}<br>
    </div>
  </div>
  <div id="footer">
    <div>
      <a href="/">Computational Vision</a> |
      <a href="http://www.ics.uci.edu/">School of Information and Computer Sciences</a> |
      <a href="http://www.uci.edu/">UC Irvine</a>
    </div>
    <div id="updated">&copy; 2007-2016 UC Irvine</div>
  </div>
  </div>
</body>
</html>

