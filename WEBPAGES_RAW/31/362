<html>
<head>
<title>Face Detection Matlab Code</title>

<script type="text/javascript">

var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-26193351-2']);
	_gaq.push(['_trackPageview']);
(function() {
var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();

</script>
</head>

<style type="text/css">
#container
{
	width : 850px;
	padding : 0px;
	margin : 25px;
	background-color : #fff;
	font-family : Ariel;
}
A:link {text-decoration: none}
A:visited {text-decoration: none}
A:active {text-decoration: none}
A:hover {text-decoration: underline}
tr {border:0px}
img.pub {height:120px;border:0px}
p.link {font-weight:bold;font-size:small;text-align:center;white-space:pre}
table.pub {width: 800px; border=0}
td.pic {text-align: right; vertical-align:top}
td.pub {width: 600px; vertical-align:bottom}
span.h2 {font-size:x-large; font-weight:bold}
span.h3 {font-size:large; font-weight:bold}
span.h3red {font-size:large; color:red}
span.h4 {font-style:italic; font-weight:bold} 
</style>

<body bgcolor=white>
<div id="container">
<center><h1><font size="6"> Face Detection, Pose Estimation and Landmark Localization in the Wild </font></h1>
<p> Last updated on 02/10/2013 </p></center>
<center><img src="AFW_results_small.jpg"></center>
<br>
<center><img src="mixture_trees.jpg"></center>
<br>
<table border="0" width="800" cellpadding="5">
<tr>
<td> <a href="http://www.ics.uci.edu/~xzhu/paper/face-cvpr12.pdf"><img src="main_cover_shadow.jpg"></a>
<td>
<p> We present a unified model for face detection, pose estimation, and landmark estimation in real-world, cluttered images. Our model is based on a mixtures of trees with a shared pool of parts; we model every facial landmark as a part and use global mixtures to capture topological changes due to viewpoint. We show that tree-structured models are surprisingly effective at  capturing global elastic deformation, while being easy to optimize unlike dense graph structures. We present extensive results on standard face benchmarks, as well as a new "in the wild" annotated dataset, that suggests our system advances the state-of-the-art, sometimes considerably, for all three tasks. Though our model is modestly trained with hundreds of faces, it compares favorably to commercial systems trained with billions of examples (such as Google Picasa and face.com).
<br>
<br>
<p> <a href="http://www.ics.uci.edu/~xzhu">X. Zhu</a>, <a href="http://www.ics.uci.edu/~dramanan">D. Ramanan</a>. <b>"Face detection, pose estimation and landmark localization in the wild"</b> <i> Computer Vision and Pattern Recognition </i>(CVPR) Providence, Rhode Island, June 2012.
<br>
<a href="http://www.ics.uci.edu/~xzhu/paper/face-cvpr12.pdf">[pdf]</a>
<br>
<a href="face.key">[slides (Keynote file,36M)]</a>
<br>
<a href="face_ppt.zip">[slides (PPT file, converted from keynote, animation may
not work correctly,17M)]</a>
</table>

<br>

<!--updates-->
<center><h3>Updates</h3> </center>
<ul>
<li> <font color="red">[NEW]</font> The AFW testdata.
<li> <font color="red">[NEW]</font> Our own landmark annotations of 400 side-view faces.
<!--<li> Added link to a C++ implementation of part-based detectors by Hilton Bristow @ WillowGarage. (08/23/2012)-->
<!--<li> Added our face models that work with Hilton's C++ implementation. (08/23/2012)-->
<li> Fixed a small bug in the basic version code when visualize the output of the independent model. (07/11/2012)
<li> Posted the two models in our CVPR2012 paper. You can use them to reproduce the results in the paper. The independent model is the best-performing one for landmark localization. (07/03/2012)
<li> Added Windows compatible mex files(.cc). (07/01/2012)
</ul>

<!-- movies -->
<center><h3>Movies</h3> <p>The following movies are <a href="http://www.divx.com/divx">DIVX</a> encoded.</p> </center>

<table style="text-align: left; margin-left: auto; margin-right: auto;" cellspacing="2" cellpadding="2">
<tbody>
<tr>
	<td style="text-align: center; vertical-align: bottom;"><a href="face_eigen_movie.avi"><img border="0" src="face_eigen_movie.jpg" alt="Lola Run Movie"></a></td>

	<td style="width: 200px; text-align: center; vertical-align: bottom;"> <a href="viewpoint_movie.avi"><img border="0" src="viewpoint_movie.jpg" alt="Lola Stop Movie"></a> </td>

</tr>
</tbody>
</table>
<!-- movies -->

<center><h3>Downloads</h3></center>
 
<table style="text-align: left"> 
<col width="200" />
<col width="500" />
<col width="100" />
<tbody><tr><th>Filename</th><th>Description</th><th>Size</th>
<tr> 
<td><a href="README">README</a>
<td>Description of contents.
<td>2.3 KB

<tr>
<!--<td><a href="face-release1.0-basic.zip">face-release1.0-basic.zip</a>-->
<td><a href="face-release1.0-basic.zip" onClick="_gaq.push(['_trackEvent', 'FaceCode', 'Download', 'BasicVersion']);">face-release1.0-basic.zip</a>
<td>Basic code (matlab) for face detection, pose and landmark estimation with pre-trained models.
<td> 8.3 MB

<tr>
<td><a href="face-release1.0-full.zip" onClick="_gaq.push(['_trackEvent', 'FaceCode', 'Download', 'FullVersion']);">face-release1.0-full.zip</a>
<td>Full code (matlab) for training and testing. You need <a href="http://www.multipie.org">MultiPIE dataset</a> to run it.
<td> 59 MB

<tr>
<td><a href="../tmp/MultiPIE_annotations.zip">MultiPIE_annotations.zip</a>
<td> Landmark annotations of multipie faces.
<td> 11 M

<!--<tr>-->
<!--<td><a href="../tmp/MultiPIE_data.zip">MultiPIE_data.zip</a>-->
<!--<td> A subset of MultiPIE Images we used to train the CVPR models.-->
<!--<td> 752 M-->

<tr>
<td><a href="mex-windows-compatible.zip">mex-windows-compatible.zip</a>
<td> Windows compatible mex files(.cc).
<td> 11 KB

<tr>
<td><a href="models_CVPR2012.zip">models_CVPR2012.zip</a>
<td> The fully shared and independent model we used to produce the curves in our CVPR2012 paper.
<td> 6.8 MB
<tr>

<!--<td><a href="http://github.com/wg-perception/PartsBasedDetector">C++ PartsBasedDetector</a>-->
<!--<td>A C++ implementation by Hilton Bristow @ <a href="http://www.willowgarage.com">WillowGarage</a>. -->
<!--<td> -->
<!--<tr>-->

<!--<td><a href="face_models.zip">Face models for Hilton's C++ implementation</a>-->
<!--<td>The face models converted from our Matlab format to Hilton's format. -->
<!--<td> 11 MB-->
<!--<tr>-->

<td><a href="AFW.zip">AFW.zip</a>
<td> The Annotated Faces in the Wild (AFW) testset.
<td> 47 MB
</tbody>
</table>

<center><h3>FAQs</h3></center>
<ul>
<li><b>Can you release the images and annotations of MultiPIE you used in the paper?</b><br>
- Unfortunately, we can't. We don't own the copyright of the data, and are not authorized to release them. You can purchase MultiPIE from <a href="http://www.flintbox.com/public/project/4742/">here</a>. The curator's contact information is also available on their website, if you have any questions regarding the data. 
<li><b>Your detector could not find faces on my image. Why?</b><br>
- The most common reason is that the faces in your image are smaller than what the released models can handle. We put three pre-learned models online. One of them (face_p146_small.mat) works for faces larger than 80x80 pixels (eyebrows to chin, ear to ear); the other two work best on faces larger than 150x150. This is simply because they were trained on large faces where all landmarks are clearly visible.<br>
If landmark is not your main concern, and you are really interested in <em>detecting small faces</em>, we recommend training your own model on smaller faces (you may need to use fewer parts accordingly). <a href="http://lrs.icg.tugraz.at/research/aflw">AFLW</a> could be a nice dataset to use for doing that.
<li><b>I saw too many false detections (or missed detections), what can I do?</b><br>
- You could try tuning the threshold saved in <em>model.thresh</em>. You may want to use a higher threshold if you see too many false detections, and a lower threshold if you miss a lot of faces.
</ul>

</div>
</body>
</html>
