<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>CS 277: Data Mining</title>
<style type="text/css">
<!--
body {
	font: 100%   Arial, Helvetica, sans-serif;
	background: #FFFFFF;
	margin: 0; /* it's good practice to zero the margin and padding of the body element to account for differing browser defaults */
	padding: 0;
	text-align: center; /* this centers the container in IE 5* browsers. The text is then set to the left aligned default in the #container selector */
	color: #000000;
}
.oneColElsCtr #container {
	width: 46em;
	background:  #FFF;
	margin: 0 auto; /* the auto margins (in conjunction with a width) center the page */
	border: 0px solid #000000;
	text-align: left; /* this overrides the text-align: center on the body element. */
}
.oneColElsCtr #mainContent {
	padding: 0 20px; /* remember that padding is the space inside the div box and margin is the space outside the div box */
}
-->
</style></head>

<body class="oneColElsCtr">

<div id="container">
  <h1 align="center">CS 277: Data Mining</h1>
  <p align="center">Spring 2013</p>
  <p><strong>Instructor:</strong> <a href="http://www.ics.uci.edu/~newman">David Newman</a></p>
  <p><strong>Lectures:</strong> 12:30 to 1:50pm, Tuesdays and Thursdays, <a href=http://www.classrooms.uci.edu/GAC/DBH1500.html>DBH 1500</a></p>
  <p><strong>Office Hours</strong>: Mondays, 11:00am - noon, DBH 4064</p>
  <p><strong>Reader</strong>: Sridevi Maharaj, sridevi.m@uci.edu</p>
  <p><strong>Message Board</strong>: <a href=https://eee.uci.edu/toolbox/messageboard/m13971/f41173/>https://eee.uci.edu/toolbox/messageboard/m13971/f41173/</a></p>
  <p><strong>Software</strong>: <a href="matlab.html">information about MATLAB</a></p>
  <p><a href="backgroundreading.html">Background Reading</a><a href="backround_reading.html"></a>: additional papers to supplement the text</p>
  <p><a href="project_guidelines.html">Project Guidelines</a>, including due dates, instructions, and links to data sets</p>


<a href="#Schedule">Schedule</a><p>

  <hr />

  <h2>New</h2><dir>
   
    <a href="homeworks/cs277_hw2.DUE_IN_CLASS_MAY21.pdf">Homework 2</a> (due in class May 21) *** Please ignore 2010 due date in pdf!<br>
    <a href="homeworks/cs277_hw2.DUE_IN_CLASS_MAY21.htm">Homework 2 info</a><br>
    <a href="homeworks/cs277_hw2.article1.pdf">Homework 2 first article to read</a><br>
    <a href="homeworks/cs277_hw2.article2.pdf">Homework 2 second article to read</a><br>
    <a href="homeworks/cs277_hw2.article3.pdf">Homework 2 third article to read</a><br>
      </dir>
      
      <hr />

  <h2><a id="Syllabus">Syllabus</a></h2><font size=-1>(follows CS 277 Syllabus <a href=http://www.ics.uci.edu/~smyth/courses/cs277/>from</a> Prof. Smyth.  Below syllabus subject to change.)</font><p>
  <h3>Introduction to Data Mining: </h3>
  <ul>
    <li><strong>Topics</strong>
<ul>
        <li>basic concepts in data mining</li>
        <li>data measurement</li>
        <li>exploratory data analysis</li>
        <li>data visualization</li>
      </ul>
    </li>
    <li><strong>Reading</strong>
      <ul>
        <li><a href="papers/mining_past.pdf">Mining the past to determine the future: problems and possibilities</a>, D. J. Hand, <em>International Journal of Forecasting</em>, 25(3), 441-451, 2009.</li>
      </ul>
    </li>
    <li><strong>Links</strong>
      <ul>
        <li><a href="http://archive.ics.uci.edu/ml/">UCI Machine Learning Repository</a> of research data sets</li>
        <li><a href="http://www.kdnuggets.com">KD nuggets</a> (industry-oriented Web site for news related to data mining)</li>
        <li>Commentary in Science, Dec 2009, on <a href="http://www-2.cs.cmu.edu/~tom/pubs/Science2009_perspective.pdf">privacy in data mining</a></li>
      </ul>
    </li>
    <li><strong>Slides</strong>
      <ul>
        <li>Introduction to Data Mining [<a href="slides/lecture1_intro.ppt">PPT</a>] [<a href="slides/lecture1_intro.pdf">PDF</a>]</li>
        <li>Measurement and Data [<a href="slides/lecture2_measurement_and_data.ppt">PPT</a>] [<a href="slides/lecture2_measurement_and_data.pdf">PDF</a>]</li>
        <li>Exploratory Data Analysis and Visualization [<a href="slides/lecture4_EDA_and_Visualization.ppt">PPT</a>] [<a href="slides/lecture4_EDA_and_Visualization.pdf">PDF</a>]</li>
      </ul>
    </li>
    <li><strong>Homeworks</strong>
      <ul>
        <li><a href="https://docs.google.com/document/d/1qeXQ3A6xiMCjNeR9bUVlT_bcauJX44HccAovgd1vAjc/edit?usp=sharing">Homework 1</a>
          <ul>
            <li><a href="homeworks/hw1.zip">Data and MATLAB files</a> for homework 1</li>
          </ul>
        </li>
        <li><a href="homeworks/cs277_hw2.DUE_IN_CLASS_MAY21.pdf">Homework 2</a>
          <ul>
            <li><a href="homeworks/cs277_hw2.DUE_IN_CLASS_MAY21.htm">data and additional information</a></li>
            <li><a href="homeworks/summarize.m">summarize.m</a> (template)</li>
            <li><a href="homeworks/define_title_strings.m">define_title_strings.m</a><br />
            </li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
  <p>&nbsp;</p>
  <h3><strong>Basic Principles of Data Mining</strong></h3>
  <ul>
    <li><strong>Topics</strong>
<ul>
        <li>predictive modeling: classification and regression</li>
        <li>model fitting as optimization</li>
        <li>evaluation of predictive performance</li>
        <li>overfitting, regularization</li>
        <li>other data mining tasks: clustering amd pattern detection</li>
      </ul>
    </li>
    <li><strong>Reading</strong>
      <ul>
        <li>other</li>
      </ul>
    </li>
    <li><strong>Links</strong>
      <ul>
        <li><a href="http://www-stat.stanford.edu/~tibs/lasso.html">Lasso method</a> for regularized regression</li>
      </ul>
    </li>
    <li><strong>Slides</strong>
      <ul>
        <li>Regression [<a href="slides/regression_slides.ppt">PPT</a>] [<a href="slides/regression_slides.pdf">PDF</a>]</li>
        <li>Classification [<a href="slides/classification_slides.ppt">PPT</a>] [<a href="slides/classification_slides.pdf">PDF</a>]</li>
      </ul>
    </li>
  </ul>
  <h3>&nbsp;</h3>
  <h3><strong>Text Mining</strong></h3>
  <ul>
    <li><strong>Topics</strong>
      <ul>
        <li>information retrieval and search</li>
        <li>text classification</li>
        <li>unsupervised learning</li>
      </ul>
    </li>
    <li><strong>Reading</strong>
      <ul>
        <li><a href="http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf">Text classification using naive Bayes</a> from  Manning, Raghavan, and Schutze</li>
        <li><a href="http://nlp.stanford.edu/IR-book/pdf/15svm.pdf">SVMs with applications to text classification</a>  from  Manning, Raghavan, and Schutze</li>
        <li>Tutorial papers on topic modeling:
          <ul>
            <li><a href="http://psiexp.ss.uci.edu/research/papers/SteyversGriffithsLSABookFormatted.pdf">Steyvers and Griffiths</a></li>
            <li><a href="http://www.cs.princeton.edu/~blei/papers/BleiLafferty2009.pdf">Blei and Lafferty</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><strong>Links</strong>
      <ul>
        <li>Mark Steyvers' <a href="http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm">MATLAB code for topic modeling</a></li>
        <li>Dave Blei's <a href="http://www.cs.princeton.edu/~blei/topicmodeling.html">page on topic modeling</a> (C code, demos, and mailing list)</li>
      </ul>
    </li>
    <li><strong>Slides</strong>
      <ul>
        <li>text classification [<a href="slides/text_classification.ppt">PPT</a>] [<a href="slides/text_classification.pdf">PDF</a>]</li>
        <li>text mining and topic models [<a href="slides/text_mining.ppt">PPT</a>] [<a href="slides/text_mining.pdf">PDF</a><a href="slides/text_mining.ppt"></a>]</li>
        <li>notes on graphical models [<a href="slides/graphical_model_intro.ppt">PPT</a>] [<a href="slides/graphical_model_intro.pdf">PDF</a>]</li>
      </ul>
    </li>
  </ul>
  <p>&nbsp;</p>
<h3><strong>Recommender Systems</strong></h3>
<ul>
  <li><strong>Topics</strong>
<ul>
  <li>recommender data, Netflix prize data</li>
  <li>nearest neighbor algorithms</li>
  <li>matrix decomposition algorithms</li>
  <li>efficient algorithms for large data sets</li>
  <li>modeling systematic effects</li>
</ul>
  </li>
  <li><strong>Reading</strong>
    <ul>
  <li>chapter in Web book</li>
  <li><a href="http://research.yahoo.net/files/ieeecomputer.pdf">Matrix factorization techniques for recommender systems</a> (2009 IEEE Computer paper by Koren et al.)</li>
  <li>Tutorial on <a href="http://www.springerlink.com/content/t87386742n752843/fulltext.pdf">collaborative filtering recommender systems</a> by Schafer et al, 2007</li>
  <li><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1167344&isnumber=26323">Recommender algorithms used by Amazon.com</a> (2003, IEEE Internet Computing article)</li>
  <li><a href="http://delivery.acm.org/10.1145/970000/963772/p5-l_herlocker.pdf?key1=963772&key2=1389554621&coll=GUIDE&dl=GUIDE&CFID=75028322&CFTOKEN=95933387">Evaluating recommender systems</a> (2004 ACM TOIS article)</li> 
    </ul>
  </li>
  <li><strong>Links</strong>
    <ul>
      <li>Proceedings of <a href="http://portal.acm.org/toc.cfm?id=1639714&coll=GUIDE&dl=GUIDE&type=proceeding&idx=SERIES11503&part=series&WantType=Proceedings&title=RecSys&CFID=77261343&CFTOKEN=44219932">2009 ACM Conference on Recommender Systems</a> (RecSys 09)</li>
    </ul>
  </li>
  <li><strong>Slides</strong>
    <ul>
      <li>Recommender systems [<a href="slides/recommender_systems.ppt">PPT</a>] [<a href="slides/recommender_systems.pdf">PDF</a>]</li>
      <li>Netflix case study [<a href="slides/netflix_overview.ppt">PPT</a>] [<a href="slides/netflix_overview.pdf">PDF</a>]</li>
      </ul>
  </li>
</ul>
<p></p>
<p></p>
<p></p>
<p></p>
<h3>&nbsp;</h3>
<h3><strong>Web Data Analysis</strong></h3>
<ul>
  <li><strong>Topics</strong>
    <ul>
      <li>Web data: collection and interpretation</li>
      <li>analyzing user browsing behavior</li>
      <li>learning from clickthrough data</li>
      <li>predictive modeling and online advertising</li>
      <li>link analysis and the PageRank algorithm</li>
    </ul>
  </li>
  <li><strong>Reading</strong>
    <ul>
      <li><a href="http://nlp.stanford.edu/IR-book/pdf/19web.pdf">Web search basics</a>, chapter 19 from Introduction to Information Retrieval by Manning, Raghavan, and Schutze</li>
      <li>PageRank and related algorithms
  <ul>
    <li><a href="http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6TYT-3WRC342-2N&amp;_user=4422&amp;_coverDate=04/30/1998&amp;_rdoc=1&amp;_fmt=high&amp;_orig=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_searchStrId=1222727636&amp;_rerunOrigin=scholar.google&amp;_acct=C000059600&amp;_version=1&amp;_urlVersion=0&amp;_userid=4422&amp;md5=50f30d8780a889bfc4fd59c1bc5c2b19">Anatomy of a large-scale hypertextual Web search engine</a>, original paper from 1998 by Brin and Page on PageRank</li>
    <li><a href="http://meyer.math.ncsu.edu/meyer/ps_files/deeperinsidepr.pdf">Deeper inside PageRank</a>, Langville and Meyer, 2004</li>
    <li><a href="http://meyer.math.ncsu.edu/meyer/ps_files/deeperinsidepr.pdf">A Survey of Eigenvector Methods for Web Information Retrieval</a>, Langville and Meyer, 2005, SIAM Review</li>
    <li><a href="http://nlp.stanford.edu/IR-book/pdf/21link.pdf">Link analysis for  the Web graph</a>, chapter 21 from Introduction to Information Retrieval by Manning, Raghavan, and Schutze</li>
  </ul>
      </li>
      <li>Cadez et al paper on <a href="http://www.springerlink.com/content/g7616r436426m686/">clustering user browsing sequences</a> with mixtures of Markov chains</li> 
      <li>Joachims paper on <a href="http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf">learning from clickthrough data</a><a href="http://www.cs.princeton.edu/~blei/papers/BleiLafferty2009.pdf"></a></li>
      </ul>
  </li>
  <li><strong>Links</strong>
    <ul>
      <li><a href="http://www2009.org/">WWW 2009 Conference and Proceedings</a></li>
      <li>Stanford class on <a href="http://www.stanford.edu/class/msande239/">"computational advertising"</a></li>
    </ul>
  </li>
  <li><strong>Slides</strong>
    <ul>
      <li>Web link analysis [<a href="slides/Web_PageRank.ppt">PPT</a>] [<a href="slides/Web_PageRank.pdf">PDF</a>]</li>
      <li>Web usage mining [<a href="slides/Web_usage_mining.ppt">PPT</a>] [<a href="slides/Web_usage_mining.pdf">PDF</a>]</li>
    </ul>
  </li>
  </ul>
<p>&nbsp;</p>
<h3><strong>Social Network Analysis</strong></h3>
<ul>
  <li><strong>Topics</strong>
    <ul>
      <li>descriptive analysis of social networks</li> 
      <li>network embedding and latent space models</li>
      <li>network data over time: dynamics and event-based networks</li>
      <li>link prediction</li>
    </ul>
  </li>
  <li><strong>Reading</strong>
    <ul>
      <li></li>
      <li>overview of network analysis methods (e.g., Newman et al)</li>
      <li>overview of SNA concepts</li>
      <li>review paper on <a href="http://sites.google.com/site/santofortunato/review_comm.pdf?attredirects=0">community detection in networks</a></li>
      <li>specific papers by Watts, Leskovecs, Barabasi, etc</li> 
    </ul>
  </li>
  <li><strong>Links</strong>
    <ul>
      <li>Class on network data analysis by Jon Kleinberg</li>
      <li>slides from Kleinberg invited talk on network algorithms</li>
      <li><a href="http://www.insna.org/">International Network for Social Network Analysis</a></li>
    </ul>
  </li>
  <li><strong>Slides</strong></li>
</ul>
<p></p>
<p></p>
<p></p> 
<hr />
<h2><a id="Schedule">Tentative Schedule</a></h2>

    <ul>
      <li>Week 1: 
	<ul>
	  <li>Tue Apr 2: Read <a href=papers/cacm12_pedro_domingos.pdf>this</a> Pedro Domingos paper
	  <li>Thu Apr 4:
	</ul>

      <li>Week 2: 
	<ul>
	  <li>Tue Apr 9: 
	  <li>Thu Apr 11: Project proposals due in class
	</ul>

      <li>Week 3: 
	<ul>
	  <li>Tue Apr 16: 
	  <li>Thu Apr 18: HW1 due in class
	</ul>

      <li>Week 4: 
	<ul>
	  <li>Tue Apr 23: 
	  <li>Thu Apr 25: 
	</ul>

      <li>Week 5: 
	<ul>
	  <li>Tue Apr 30: Project progress presentation in class
	  <li>Thu May 2: Project progress presentation in class
	</ul>

      <li>Week 6: 
	<ul>
	  <li>Tue May 7: 
	  <li>Thu May 9:
	</ul>

      <li>Week 7: 
	<ul>
	  <li>Mon May 13: Office hours cancelled
	  <li>Tue May 14: ***NO LECTURE*** Continue work on project
	  <li>Thu May 16: 
	</ul>

      <li>Week 8: 
	<ul>
	  <li>Tue May 21: Scientific writing.  HW2 DUE IN CLASS
	  <li>Thu May 23: Scientific writing
	</ul>

      <li>Week 9: 
	<ul>
	  <li>Tue May 28: Project presentations in class
	  <li>Thu May 30: Project presentations in class
	</ul>

      <li>Week 10: 
	<ul>
	  <li>Tue June 4: Project presentations in class
	  <li>Thu June 6: Project presentations in class; Final project report due in class
	</ul>

    </ul>

<hr />
<h2>Grading</h2>

<p>Your class grade will be based on a class project (70%) and two
homeworks (total of 30%, each HW is worth 15%). The projects will
require submission of a progress report during the quarter, a
presentation in class during Weeks 9 or 10, and a final report.</p>

<hr />
<h2>Academic Honesty</h2>
<p>It is the responsibility of each student to be familiar with the <a href=http://honesty.uci.edu/>UCI Senate Academic Honesty Policies</a>. For homework assignments and projects you are allowed to discuss ideas and concepts verbally with other class members, but you are not allowed to look at or copy anyone else's written solutions or code relating to homework assignments or projects. All material submitted must be material you have personally written during this quarter. Failure to adhere to this policy can result in a student receiving a failing grade in the class. </p>
  <div id="mainContent">
    <!-- end #mainContent -->
  </div>
<!-- end #container --></div>
</body>
</html>
