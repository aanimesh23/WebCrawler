# jemdoc: menu{MENU}{index.html}, showsource
 = NIPS '13 Workshop on Crowdsourcing: Theory, Algorithms and Applications
\n
in conjunction with [http://nips.cc/Conferences/2013/ NIPS] 2013. 

~~~
 *Important Dates * {{<img src="pics/MSR_3.png" style="float:right" alt="MSR" height="100">}} 
- Submission deadline: October9, 2013 \n  
- Acceptance Notification: October 23, 2013 \n  
- Workshop: December 9, 2013 \n  
{{<!-- - [schedule.html {{<font color="blue">[Schedule]</font>}}] ~ [keynotes.html {{<font color="blue">[Keynotes]</font>}}] ~  [index.html\#Papers {{<font color="blue">[Papers]</font>}}]  -->}}
~~~

== {{<a name='Overview'> Overview </a> }}

All machine learning systems are an integration of data that store human or physical knowledge, and algorithms that discover knowledge patterns and make predictions to new instances. Even though most research attention has been focused on developing more efficient learning algorithms,  it is the quality and amount of training data that predominately govern the performance of real-world systems. This is only amplified by the recent popularity of large scale and complicated learning systems such as deep networks, which require millions to billions of training data to perform well. Unfortunately, the traditional methods of collecting data from specialized workers are usually expensive and slow. In recent years, however, the situation has dramatically changed with the emergence of crowdsourcing, where huge amounts of labeled data are collected from large groups of (usually online) workers for low or no cost. Many machine learning tasks, such as computer vision and natural language processing are increasingly benefitting from data crowdsourced platforms such as Amazon Mechanical Turk and CrowdFlower. On the other hand, tools in machine learning, game theory and mechanism design can help to address many challenging problems in crowdsourcing systems, such as making them more reliable, efficient and less expensive. 
 
In this workshop, we call attention back to sources of data, discussing cheap and fast data collection methods based on crowdsourcing, and how it could impact subsequent machine learning stages. 
Furthermore, we will emphasize how the data sourcing paradigm interacts with the most recent emerging trends of machine learning in NIPS community. 

Examples of topics of potential interest in the workshop include (but are not limited to): 

 \n
- Application of crowdsourcing to machine learning. 

- Reliable crowdsourcing, e.g., label aggregation, quality control. 

- Optimal budget allocation or active learning in crowdsourcing.

- Pricing and incentives in crowdsourcing markets. 

- Workflow design and answer aggregation for complex tasks (e.g., machine translation, proofreading). 

- Prediction markets / information markets and its connection to learning. 

- Theoretical analysis for crowdsourcing algorithms, e.g., error rates and sample complexities for label aggregation or budget allocation algorithms.

{{<!--  == {{<a name='Speakers'> Invited Speakers </a> }}  TBD -->}}

== {{<a name='CFP'>  Call for Papers </a> }}

Submissions should follow the [http://nips.cc/Conferences/2013/PaperInformation/AuthorSubmissionInstructions NIPS format] and are encouraged to be up to eight pages plus an additional reference page. {{<!--plus an additional fifth page for references.-->}} 
Papers submitted for review do not need to be anonymized. 
There will be no official proceedings, but the accepted papers will be made available on the workshop website. Accepted papers will be either presented as a talk or poster. 

We welcome submissions both on novel research work as well as extended abstracts on work recently published or under review in another conference or journal (please state the venue of publication in the later case); we particularly encourage submission of visionary position papers on the emerging trends of the field. 

Please submit papers in PDF format [XXX here]. 
{{<!--For questions or problems, please email lqiang67+MLcrowd_workshop AT gmail.com. -->}}
 
== {{<a name='Organizers'> Organizers </a> }}

- [http://research.microsoft.com/en-us/um/people/denzho/ Dengyong Zhou], 
[http://www.cs.ucla.edu/~jenn/  Jenn W. Vaughan], 
[http://research.microsoft.com/en-us/um/people/nikdev/ Nikhil R. Devanur].
Microsoft Research, Redmond 

- [http://www.ics.uci.edu/~qliu1/ Qiang Liu], 
[http://www.ics.uci.edu/~qliu1/ Alexander Ihler]. 
UC Irvine

- [http://www.cs.cmu.edu/~xichen/ Xi Chen]. UC Berkeley \& NYU 


\n
\n
\n
\n
== {{<a name='Related'> Related Workshops, Conferences and Resources </a> }} 

- [http://www.ics.uci.edu/~qliu1/MLcrowd_ICML_workshop/ ICML 2013 Workshop on Machine Learning Meets Crowdsourcing.]
- [http://crowdml12.wordpress.com ICML 2012 Workshop on Machine Learning in Human Computation \& Crowdsourcing.]
- [https://sites.google.com/site/comblearn/program ICML 2011 Workshop on Combining Learning Strategies to Reduce Label Cost.]
- [http://nips.cc/Conferences/2012/Program/event.php?ID=3140 NIPS 2012 Workshop on Human Computation for Science and Computational Sustainability. ]
- [http://nips.cc/Conferences/2011/Program/event.php?ID=2522 NIPS 2011 Workshop on Computational Social Science and the Wisdom of Crowds.]
- [http://nips.cc/Conferences/2010/Program/event.php?ID=1990 NIPS 2010 Workshop on Computational Social Science and the Wisdom of Crowds.]
- [http://filebox.ece.vt.edu/~parikh/acvhl2010.htm CVPR 2010 Workshop on Advancing Computer Vision with Humans in the Loop (ACVHL)]
- [http://www.humancomputation.com/2013/ Conference on Human Computation \& Crowdsourcing (HCOMP), 2013.]
- [http://www.humancomputation.com/2012/About_the_Workshop.html 1st-4th Human Computation Workshop (HCOMP).]
- [http://crowdresearch.org/crowdcamp/indexCHI2012.html/ CrowdCamp 2012], [http://crowdresearch.org/crowdcamp/ 2012].
- [http://crowdresearch.org/chi2011-workshop/ CHI 2011 Workshop on Crowdsourcing and Human Computation, 2013]
- See more information on  [http://crowdresearch.org CrowdResearch.org] or [http://ir.ischool.utexas.edu/crowd/ Mathew Lease's crowdsourcing site.]
