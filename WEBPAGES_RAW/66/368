<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />
<title>Winter 2014 | Center for Machine Learning and Intelligent Systems</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="http://cml.ics.uci.edu/xmlrpc.php" />
<!--[if lt IE 9]>
<script src="http://cml.ics.uci.edu/wp-content/themes/bonpress-wpcom/js/html5.js" type="text/javascript"></script>
<![endif]-->

<link rel='dns-prefetch' href='//s.w.org' />
<link rel="alternate" type="application/rss+xml" title="Center for Machine Learning and Intelligent Systems &raquo; Feed" href="http://cml.ics.uci.edu/feed/" />
<link rel="alternate" type="application/rss+xml" title="Center for Machine Learning and Intelligent Systems &raquo; Comments Feed" href="http://cml.ics.uci.edu/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Center for Machine Learning and Intelligent Systems &raquo; Winter 2014 Comments Feed" href="http://cml.ics.uci.edu/2014/01/winter-2014/feed/" />
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.2.1\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.2.1\/svg\/","svgExt":".svg","source":{"concatemoji":"http:\/\/cml.ics.uci.edu\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.7.2"}};
			!function(a,b,c){function d(a){var b,c,d,e,f=String.fromCharCode;if(!k||!k.fillText)return!1;switch(k.clearRect(0,0,j.width,j.height),k.textBaseline="top",k.font="600 32px Arial",a){case"flag":return k.fillText(f(55356,56826,55356,56819),0,0),!(j.toDataURL().length<3e3)&&(k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57331,65039,8205,55356,57096),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57331,55356,57096),0,0),c=j.toDataURL(),b!==c);case"emoji4":return k.fillText(f(55357,56425,55356,57341,8205,55357,56507),0,0),d=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55357,56425,55356,57341,55357,56507),0,0),e=j.toDataURL(),d!==e}return!1}function e(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g,h,i,j=b.createElement("canvas"),k=j.getContext&&j.getContext("2d");for(i=Array("flag","emoji4"),c.supports={everything:!0,everythingExceptFlag:!0},h=0;h<i.length;h++)c.supports[i[h]]=d(i[h]),c.supports.everything=c.supports.everything&&c.supports[i[h]],"flag"!==i[h]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[i[h]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel='stylesheet' id='bonpress-style-css'  href='http://cml.ics.uci.edu/wp-content/themes/bonpress-cml/style.css?ver=4.7.2' type='text/css' media='all' />
<link rel='stylesheet' id='tipsy-css'  href='http://cml.ics.uci.edu/wp-content/plugins/cml/wp-shortcode/css/tipsy.css?ver=4.7.2' type='text/css' media='all' />
<link rel='stylesheet' id='mts_wpshortcodes-css'  href='http://cml.ics.uci.edu/wp-content/plugins/cml/wp-shortcode/css/wp-shortcode.css?ver=4.7.2' type='text/css' media='all' />
<script type='text/javascript' src='http://cml.ics.uci.edu/wp-includes/js/jquery/jquery.js?ver=1.12.4'></script>
<script type='text/javascript' src='http://cml.ics.uci.edu/wp-includes/js/jquery/jquery-migrate.min.js?ver=1.4.1'></script>
<script type='text/javascript' src='http://cml.ics.uci.edu/wp-content/plugins/cml/wp-shortcode/js/jquery.tipsy.js?ver=4.7.2'></script>
<script type='text/javascript' src='http://cml.ics.uci.edu/wp-content/plugins/cml/wp-shortcode/js/wp-shortcode.js?ver=4.7.2'></script>
<link rel='https://api.w.org/' href='http://cml.ics.uci.edu/wp-json/' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://cml.ics.uci.edu/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://cml.ics.uci.edu/wp-includes/wlwmanifest.xml" /> 
<link rel='prev' title='Fall 2013' href='http://cml.ics.uci.edu/2013/09/fall-2013/' />
<link rel='next' title='ICS grad students take second place in sbv IMPROVER competition' href='http://cml.ics.uci.edu/2014/02/2014_improver/' />
<meta name="generator" content="WordPress 4.7.2" />
<link rel="canonical" href="http://cml.ics.uci.edu/2014/01/winter-2014/" />
<link rel='shortlink' href='http://cml.ics.uci.edu/?p=516' />
<link rel="alternate" type="application/json+oembed" href="http://cml.ics.uci.edu/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fcml.ics.uci.edu%2F2014%2F01%2Fwinter-2014%2F" />
<link rel="alternate" type="text/xml+oembed" href="http://cml.ics.uci.edu/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fcml.ics.uci.edu%2F2014%2F01%2Fwinter-2014%2F&#038;format=xml" />
</head>

<body class="post-template-default single single-post postid-516 single-format-standard">
<div id="page" class="hfeed site">
		<header id="masthead" class="all-header" role="banner">
		<hgroup class="hgroup-wide">
                        <a href="http://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home"><img src='/wp-content/uploads/cml/cml-curve.jpg'></a>
<!--			<h1 class="site-title"><a href="http://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home">Center for Machine Learning and Intelligent Systems</a></h1>
			<h2 class="site-description">University of California, Irvine</h2> -->
			<h1 class="site-title"><a href="http://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home">Center for Machine Learning and Intelligent Systems</a></h1>
			<h2 class="site-description">Bren School of Information and Computer Science</h2>			
			<h2 class="site-description">University of California, Irvine</h2>
			<div style="clear:both"></div>
		</hgroup>
	</header>
	<header id="masthead" class="site-header" role="banner">
		<hgroup class="hgroup-img">
                        <a href="http://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home"><img src='/wp-content/uploads/cml/cml-curve.jpg'></a>
			<h1 class="site-title"><a href="http://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home">Center for Machine Learning and Intelligent Systems</a></h1>
			<h2 class="site-description">University of California, Irvine</h2>
		</hgroup>

		<nav id="site-navigation" class="navigation-main" role="navigation">
			<h1 class="menu-toggle">Menu</h1>
			<div class="screen-reader-text skip-link"><a href="#content" title="Skip to content">Skip to content</a></div>

			<div class="menu-navigation-container"><ul id="menu-navigation" class="menu"><li id="menu-item-234" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-234"><a href="http://cml.ics.uci.edu/">Home</a></li>
<li id="menu-item-79" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-79"><a href="http://cml.ics.uci.edu/home/about-us/">About CML</a>
<ul class="sub-menu">
	<li id="menu-item-78" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-78"><a href="http://cml.ics.uci.edu/home/about-us/">About us</a></li>
	<li id="menu-item-429" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-429"><a href="http://cml.ics.uci.edu/category/news/">News</a></li>
	<li id="menu-item-76" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-76"><a href="http://cml.ics.uci.edu/home/contact-us/">Contact Us</a></li>
</ul>
</li>
<li id="menu-item-539" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-539"><a>People</a>
<ul class="sub-menu">
	<li id="menu-item-55" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-55"><a href="http://cml.ics.uci.edu/faculty/">Faculty</a></li>
	<li id="menu-item-220" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-220"><a href="http://cml.ics.uci.edu/alumni/">Alumni</a></li>
</ul>
</li>
<li id="menu-item-75" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-75"><a href="http://cml.ics.uci.edu/aiml/">Events &#038; Seminars</a>
<ul class="sub-menu">
	<li id="menu-item-74" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-74"><a href="http://cml.ics.uci.edu/aiml/">AI/ML Seminar Series</a></li>
	<li id="menu-item-73" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-73"><a href="http://cml.ics.uci.edu/aiml/ml-reading-group/">ML Reading Group</a></li>
</ul>
</li>
<li id="menu-item-222" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-222"><a>Education &#038; Resources</a>
<ul class="sub-menu">
	<li id="menu-item-227" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-227"><a href="http://cml.ics.uci.edu/courses/">Courses</a></li>
	<li id="menu-item-221" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-221"><a href="http://cml.ics.uci.edu/books/">Books</a></li>
</ul>
</li>
<li id="menu-item-81" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-81"><a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI Machine Learning Archive</a></li>
<li id="menu-item-87" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-87"><a href="http://cml.ics.uci.edu/sponsors-funding/">Sponsors &#038; Funding</a></li>
<li id="menu-item-86" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-86"><a href="http://cml.ics.uci.edu/subscribe/">Subscribe to CML List</a></li>
</ul></div>		</nav><!-- #site-navigation -->
	</header><!-- #masthead -->


	<div id="primary" class="content-area">
		<div id="content" class="site-content" role="main">

		
			
<article id="post-516" class="post-516 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<h1 class="entry-title">Winter 2014</h1>		<span class="entry-format genericon">Standard</span>
		<div class="entry-meta">
			<a href="http://cml.ics.uci.edu/2014/01/winter-2014/" title="3:33 pm" rel="bookmark"><time class="entry-date genericon" datetime="2014-01-01T15:33:27+00:00">January 1, 2014</time></a>
			
			<span class="cat-links genericon"><a href="http://cml.ics.uci.edu/category/aiml/" rel="category tag">AIML</a></span>
					</div>
	</header><!-- .entry-header -->

	<div class="entry-content">
		<br />
<table cellpadding=5 border=1>
<col width="100">
<col>
<p>  <!-- ==== Jan 13 =================================== --> </p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Jan 13</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://homepages.cwi.nl/~pdg/"><b>Peter Grunwald </b></a><br />Coordinator<br />Information-theoretic Learning Group<br />Centrum voor Wiskunde en Informatica (CWI) &#038; Leiden University</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Learning the learning rate: how to repair Bayes when the model is wrong </b></a></span></div><div class="togglec clearfix">Bayesian inference can behave badly if the model under consideration is wrong yet useful: the posterior may fail to concentrate even in the large sample limit.  We demonstrate this using a simple linear regression example. We then introduce a test that can tell from the data whether we are heading for such a situation. If we are, we adjust the learning rate (equivalently: make the prior lighter-tailed, or penalize the likelihood more) in a data-dependent way. The resulting &#8220;safe-Bayesian&#8221; estimator continues to achieve good rates with wrong models. In classification, it learns faster in easy settings, i.e.  when a Tsybakov condition holds. The safe estimator is based on empirical mixability/exp-concavity, which generalizes an idea from worst-case online prediction. Thus, safe estimation connects three paradigms: Bayesian inference, (frequentist) statistical learning theory and (individual sequence) on-line prediction.</p>
<p>For an informal introduction to the idea, see Larry Wasserman&#8217;s blog: http://normaldeviate.wordpress.com/2012/06/26/self-repairing-bayesian-inference/</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Jan 20 =================================== --> </p>
<tr>
<td valign=top class='aiml-none'>
<div class="aiml-date"><b>Jan 20</b></div>
</td>
<td valign=top class='aiml-none'>
<div class="aiml-name"><b>Martin Luther King, Jr Day</b><br />(no seminar)</div>
<p>
  </td>
</tr>
<p>  <!-- ==== Jan 27 =================================== --> </p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Jan 27</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www-bcf.usc.edu/~feisha/"><b>Fei Sha</b></a><br />Assistant Professor<br />Department of Computer Science<br />University of California, Los Angeles</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Learning kernels for visual domain adaptation</b></a></span></div><div class="togglec clearfix">Statistical machine learning has become an important driving force behind many  application fields.  By large, however, its theoretical underpinning has hinged on the stringent assumption that the learning environment is stationary. In particular, the data distribution on which statistical models are optimized is the same as the distribution to which the models are applied.</p>
<p> Real-world applications are far more complex than the pristine condition. For instance, computer vision systems for recognizing objects in images often suffer from significant performance degradation if they are evaluated on image datasets that are different from the dataset on which they are designed.</p>
<p> In this talk, I will describe our efforts in addressing this important challenge of building intelligent systems that are robust to distribution disparity. The central theme is to learn invariant features, cast as learning kernel functions and adapt probabilistic models across different distributions (i.e., domains). To this end, our key insight is to discover and exploit hidden structures in the data. These structures, such as manifolds and discriminative clusters, are intrinsic and thus resilient to distribution changes due to exogenous factors. I will present several learning algorithms we have proposed and demonstrate their effectiveness in pattern recognition tasks from computer vision.</p>
<p> This talk is based on the joint work with my students (Boqing Gong and Yuan Shi, both from USC) and our collaborator Prof. Kristen Grauman (U. of Texas, Austin).</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Feb 3 =================================== --> </p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Feb 3</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www.ics.uci.edu/~rcammaro/"><b>Rosario Cammarota</b></a><br />System Security Architect<br />Qualcomm Research</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Automatic construction of program optimization strategies</b></a></span></div><div class="togglec clearfix">An optimization strategy is a mean to improve program performance, e.g., through architectural, execution and development environment enhancements (including new architectural mechanisms/instructions, the use of specialized libraries, new compiler optimizations or compiler optimization sequences, new algorithms). Constructing complex optimization strategies by composition of simpler optimizations has been shown to provide significant performance improvements to programs. However, composing simpler optimizations is non-trivial because (a) the combination of possibilities that are available is large and (b) the fact that the effect of the interplay of basic optimizations to program performance is difficult to predict.  This talk will first briefly survey previous work on the application of machine learning techniques to construct program optimization strategies and second proposes a practical and widely applicable technique to construct program optimization strategies based on recommendation systems. Preliminary results are shown to support that the proposed technique is equally applicable to several compelling performance evaluation studies, including characterization, comparison and tuning of hardware configurations, compilers, run-time environments or any combination thereof.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Feb 7 =================================== --> </p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Feb 7</b><br />Bren Hall 4011<br />1pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www.ml.inf.ethz.ch/people/professors/jbuhmann"><b>Joachim M. Buhmann</b></a><br />Professor<br />Computer Science Department<br />ETH Zurich</div>
<div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>What is the information content of an algorithm?</b></a></span></div><div class="togglec clearfix">Algorithms are exposed to randomness in the input or noise during the computation. How well can they preserve the information in the data w.r.t. the output space? Algorithms especially in Machine Learning are required to generalize over input fluctuations or randomization during execution.  This talk elaborates a new framework to measure the &#8220;informativeness&#8221; of algorithmic procedures and their &#8220;stability&#8221; against noise.  An algorithm is considered to be a noisy channel which is characterized by a generalization capacity (GC). The generalization capacity objectively ranks different algorithms for the same data processing task based on the bit rate of their respective capacities. The problem of grouping data is used to demonstrate this validation principle for clustering algorithms, e.g. k-means, pairwise clustering, normalized cut, adaptive ratio cut and dominant set clustering. Our new validation approach selects the most informative clustering algorithm, which filters out the maximal number of stable, task-related bits relative to the underlying hypothesis class. The concept also enables us to measure how many bit are extracted by sorting, feature selection or minimum spanning tree algorithms when the respective inputs are contaminated by noise.</p>
<p><b>Bio:</b>Joachim M. Buhmann leads the Machine Learning Laboratory in the Department of Computer Science at ETH Zurich. He has been a full professor of Information Science and Engineering since October 2003.  He studied physics at the Technical University Munich and obtained his PhD in Theoretical Physics. As postdoc and research assistant professor, he spent 1988-92 at the University of Southern California, Los Angeles, and the Lawrence Livermore National Laboratory. He held a professorship for applied computer science at the University of Bonn, Germany from 1992 to 2003.  His research interests spans the areas of pattern recognition and data analysis, including machine learning, statistical learning theory and information theory. Application areas of his research include image analysis, medical imaging, acoustic processing and bioinformatics. Currently, he serves as president of the German Pattern Recognition Society.</div></div><div class="clear"></div>
<p>Co-sponsored with the <a href="http://www.igb.uci.edu/">Institute for Genomics and Bioinformatics</a>
  </td>
</tr>
<p>  <!-- ==== Feb 10 =================================== --> </p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Feb 10</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www.ics.uci.edu/~agelfand/"><b>Andrew Gelfand</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>On Max-Product BP, MAP inference and Weighted Matchings</b></a></span></div><div class="togglec clearfix">The problem of finding the most probable, or MAP, configuration of a graphical model can also be cast as an integer linear programming (ILP) problem. Formulating the MAP problem as an ILP has led to the development of many approximate inference methods based on linear programming (LP) relaxations, including Tree-Reweighted Belief Propagation, MPLP and Max-Sum Diffusion.  Recent work suggests that going in the opposite direction and posing an ILP as a MAP problem may also prove beneficial. In this talk, I&#8217;ll focus on a classic combinatorial optimization problem, the weighted matching problem, and show that if it is posed as a MAP inference problem then the Max-Product Belief Propagation (BP) algorithm always converges to the MAP configuration. Remarkably, this is true even though the weighted matching graphical model is loopy and BP is neither guaranteed to converge, nor be correct in such models. I&#8217;ll then discuss a cutting plane approach to solving the weighted matching problem that utilizes the BP algorithm to iteratively tighten the LP relaxation of the matching ILP. This line of work improves our understanding of the performance of the loopy BP algorithm in general and further strengthens the theoretical link between message passing algorithms and optimization theory.</p>
<p>This talk is based on joint work with Misha Chertkov (Los Alamos National Lab) and Jinwoo Shin (KAIST University).</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Feb 17 =================================== --> </p>
<tr>
<td valign=top class='aiml-none'>
<div class="aiml-date"><b>Feb 17</b></div>
</td>
<td valign=top class='aiml-none'>
<div class="aiml-name"><b>President&#8217;s Day</b><br />(no seminar)</div>
<p>
  </td>
</tr>
<p>  <!-- ==== Feb 24 =================================== --> </p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Feb 24</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www.ics.uci.edu/~mlevorat/"><b>Marco Levorato</b></a><br />Assistant Professor<br />Department of Computer Science<br />University of California, Irvine</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Sparse approximation methods for large scale network control</b></a></span></div><div class="togglec clearfix">The explosion of the number of devices capable of transmitting and receiving information connected to wireless and wired communication infrastructures, both for human-to-human and machine-to-machine interaction, is forcing a technological transition to a different inter-networking model. The communication infrastructure is merging from a collection of distinct networks to a single super-network deeply integrated with heterogeneous sub-systems. Mobile health-care, smart energy grids and social networks are just examples of complex systems interoperating with the global communication infrastructure. Adaptive learning and control are the key to empower the next generation of communication networks with the ability to operate and interoperate in heterogeneous environments and with heterogeneous sub-systems. However, the high complexity of these systems discourages the use of traditional tools in practice. In this seminar, I will discuss a novel framework for online learning and optimization in complex inter-networked systems based on sparse approximation theory and wavelet analysis. The key observation is that communication protocols, and supposedly natural and human phenomena, induce a structured behavior of the stochastic process tracking the state of the system. This induces a regular structure at different time scales (hop number) of the graph modeling state transitions, and enables dimensionality reduction based on graph wavelet analysis and graph filtering. Sparse approximation algorithms can be then employed to dramatically reduce the number of observations needed to estimate fundamental control functions mapped on the state space of the observed system.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Mar 3 =================================== --> </p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Mar 3</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www.dmi.usherb.ca/~larocheh/index_en.html"><b>Hugo La Rochelle</b></a><br />Assistant Professor<br />Department of Computer Science<br />Université de Sherbrooke</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Deep Learning for Distribution Estimation</b></a></span></div><div class="togglec clearfix">Deep learning methods have shown to be powerful approaches for modelling a large variety of data (speech, computer vision, natural language, biological, etc.) and solve a vast range of machine learning tasks (classification, regression, etc.). In this talk, I will present my recent research on using deep neural networks for the task of distribution/density estimation, one of the most fundamental problem in machine learning. Specifically, I will discuss the neural autoregressive distribution estimator (NADE), a state-of-the-art estimator of the probability distribution of data. I will then describe a deep version of NADE, which again illustrates the statistical modelling power of deep models.</p>
<p><b>Bio:</b>Hugo Larochelle is Assistant Professor at the Université de Sherbrooke (UdeS). Before joining the Computer Science department of UdeS in 2011, he spent two years in the machine learning group at University of Toronto, as a postdoctoral fellow under the supervision of Geoffrey Hinton. He obtained his Ph.D. at Université de Montréal, under the supervision of Yoshua Bengio. He is the recipient of two Google Faculty Awards, acts as associate editor for the IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) and is a member of the editorial board of the Journal of Artificial Intelligence Research (JAIR).</p>
<p><i>This talk co-sponsored with the Institute for Genomics and Bioinformatics.</i></div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Mar 10 =================================== --> </p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Mar 10</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www.ics.uci.edu/~jfoulds/"><b>James Foulds</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Fast and Accurate Inference and Evaluation for Topic Models</b></a></span></div><div class="togglec clearfix">Statistical topic models such as latent Dirichlet allocation (LDA) have become a standard tool for analyzing text corpora, with broad applications to political science, the humanities, sociology, conversational dialog, and more.   In recent years there has been an explosion in the amount of digital text information available, leading to challenges of scale for traditional inference algorithms for topic models.  In this talk, I will present algorithms for learning and evaluating topic models, quickly, accurately and at scale.</p>
<p> In part one of the talk I will describe SCVB0, a stochastic variational Bayesian inference algorithm which exploits the collapsed representation of LDA.  The algorithm can fit a topic model to Wikipedia in a matter of hours, and to the NIPS corpus of machine learning articles in seconds.  It is also extremely simple and easy to implement.</p>
<p> The second part of the talk considers the problem of evaluating topic models by computing the log-likelihood of held-out data, a task which requires approximating an intractable high-dimensional integral.  Annealed importance sampling (AIS), a Monte Carlo integration technique which operates by annealing between two distributions, has previously been successfully used to evaluate topic models.  We introduce new annealing paths which exhibit much lower empirical variance than the previous AIS approach (albeit at the cost of increased bias when given too few iterations), facilitating reliable per-document comparisons of topic models. We then show how to use these paths to evaluate the predictive performance of topic model learning algorithms on a per-iteration basis. The proposed method achieves better estimates at this task than previous algorithms, in some cases with an order of magnitude less computational effort.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Mar 17 =================================== --> </p>
<tr>
<td valign=top class='aiml-none'>
<div class="aiml-date"><b>Mar 17</b></div>
</td>
<td valign=top class='aiml-none'>
<div class="aiml-name"><b>Finals Week</b><br />(no seminar)</div>
<p>
  </td>
</tr>
</table>
			</div><!-- .entry-content -->

	</article><!-- #post-## -->


				<nav role="navigation" id="nav-below" class="navigation-post">
		<h1 class="screen-reader-text">Post navigation</h1>

	
		<div class="nav-previous"><a href="http://cml.ics.uci.edu/2013/09/fall-2013/" rel="prev"><span class="meta-nav">&larr;</span> Fall 2013</a></div>		<div class="nav-next"><a href="http://cml.ics.uci.edu/2014/02/2014_improver/" rel="next">ICS grad students take second place in sbv IMPROVER competition <span class="meta-nav">&rarr;</span></a></div>
	
	</nav><!-- #nav-below -->
	
			
		
		</div><!-- #content -->
	</div><!-- #primary -->

	<div id="secondary" class="widget-area" role="complementary">
				<aside id="search-2" class="widget widget_search">	<form method="get" id="searchform" class="searchform" action="http://cml.ics.uci.edu/" role="search">
		<label for="s" class="screen-reader-text">Search</label>
		<input type="search" class="field" name="s" value="" id="s" placeholder="Search &hellip;" />
		<input type="submit" class="submit" id="searchsubmit" value="Search" />
	</form>
</aside>	</div><!-- #secondary -->

</div><!-- #page -->

<footer id="colophon" class="site-footer" role="contentinfo">
<p style="text-align:center;margin:0;">(c) 2015 <a href="http://cml.ics.uci.edu">Center for Machine Learning and Intelligent Systems</a>
	<div class="site-info">
				<a href="http://wordpress.org/" rel="generator">WordPress</a>/<a href="http://www.wpzoom.com/">BonPress</a>
	</div><!-- .site-info -->
</footer><!-- #colophon -->

<script type='text/javascript' src='http://cml.ics.uci.edu/wp-content/themes/bonpress-wpcom/js/navigation.js?ver=20120206'></script>
<script type='text/javascript' src='http://cml.ics.uci.edu/wp-content/themes/bonpress-wpcom/js/skip-link-focus-fix.js?ver=20130115'></script>
<script type='text/javascript' src='http://cml.ics.uci.edu/wp-includes/js/wp-embed.min.js?ver=4.7.2'></script>

</body>
</html>
